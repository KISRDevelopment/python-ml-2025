{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b87f0538-81cb-4239-98ec-6e69fbb68119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import jax \n",
    "import jax.numpy as jnp \n",
    "import pandas as pd \n",
    "import sklearn.metrics\n",
    "import copy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9525fc3-f756-4a87-b8b2-edc2fbaaee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs(input_df):\n",
    "    \"\"\"\n",
    "        Prepares the input features that will be fed into the model.\n",
    "\n",
    "        Inputs:\n",
    "            input_df: the input dataframe into the function. Should consist ONLY of input features.\n",
    "        Outputs:\n",
    "            Z: the input feature matrix of size NxK, where K is the number of features\n",
    "    \"\"\"\n",
    "    # Let's identify categorical columns in a dataframe\n",
    "    categorical_cols = input_df.select_dtypes(include='object').columns\n",
    "    \n",
    "    # Let's identify the numeric columns in the dataframe\n",
    "    numeric_cols = input_df.select_dtypes(include='number').columns\n",
    "\n",
    "    # We want to construct the input features into the model\n",
    "    # We will use a numpy array that contains both numeric and categorically encoded values\n",
    "    X = input_df[numeric_cols].to_numpy() # (NxK)\n",
    "    \n",
    "    # Now we need to z-score the numeric features so that they can lead to efficient learning\n",
    "    col_means = np.mean(X, axis=0) # K\n",
    "    col_stds = np.std(X, axis=0, ddof=1) # K\n",
    "    \n",
    "    # Z-score\n",
    "    # (NxK - \n",
    "    #  1xK) \n",
    "    #  / \n",
    "    #  (1xK)\n",
    "    Z = (X - col_means[None, :]) / col_stds[None, :]\n",
    "    \n",
    "    # Now we want to code the categorical columns using one-hot encoding\n",
    "    for col in categorical_cols:\n",
    "        # NxC (C is the number of unique values in the column)\n",
    "        # So for origin this will be Nx3 \n",
    "        dummies = pd.get_dummies( input_df[col] ).to_numpy() \n",
    "        \n",
    "        # concatenate dummies matrix onto Z\n",
    "        #print(Z.shape)\n",
    "        #print(dummies.shape)\n",
    "        Z = np.hstack((Z, dummies)) \n",
    "    \n",
    "    # finally we want to add a column of ones at the start of Z\n",
    "    ones_col = np.ones((Z.shape[0], 1)) # Nx1\n",
    "    \n",
    "    Z = np.hstack((ones_col, Z))\n",
    "\n",
    "    return Z\n",
    "\n",
    "def forward_fn(params, Z):\n",
    "    \"\"\"\n",
    "        The MLP forward function.\n",
    "        This is a neuron network with one hidden layer and ReLU activations.\n",
    "        \n",
    "        Inputs:\n",
    "            params: the weight of the model\n",
    "            Z: the input feature matrix, as returned by prepare_inputs (size NxK)\n",
    "        Output:\n",
    "            yhat: the model's predictions (size N)\n",
    "    \"\"\"\n",
    "    # compute the inputs into the neurons\n",
    "    # 1xH + Nxd @ dxH = 1xH + NxH = NxH\n",
    "    hidden_inputs = params['b_hidden'][None,:] + Z @ params['W_input_hidden']\n",
    "    \n",
    "    # next, we apply the non-linearity to those inputs\n",
    "    #hidden_activations = jnp.tanh(hidden_inputs) # NxH\n",
    "    hidden_activations = jnp.maximum(0, hidden_inputs) # <-- ReLU\n",
    "    \n",
    "    # now we compute the output\n",
    "    # 1 + NxH @ H = 1 + N = N \n",
    "    f = params['b_output'] + hidden_activations @ params['W_hidden_output']\n",
    "    p = 1/(1+jnp.exp(-f)) # <-- sigmoid function (the probability of a positive)\n",
    "\n",
    "    # so that the log doesn't blow up\n",
    "    p = jnp.clip(p, 0.01, 0.99)\n",
    "\n",
    "    return p\n",
    "\n",
    "def predict(params, input_df):\n",
    "    \"\"\"\n",
    "        Convienience function that prepares inputs and runs the forward function.\n",
    "\n",
    "        Inputs:\n",
    "            params: the weights of the model\n",
    "            input_df: input data frame (input features only, no output column).\n",
    "        Output:\n",
    "            yhat: the model's predictions (size N)\n",
    "    \"\"\"\n",
    "    Z = prepare_inputs(input_df)\n",
    "    return forward_fn(params, Z)\n",
    "\n",
    "def loss_fn(params, Z, y, penalty):\n",
    "    \"\"\"\n",
    "        Computes the CE loss function for the model.\n",
    "\n",
    "        Inputs:\n",
    "            params: the weights of the model\n",
    "            Z: the input feature matrix, as returned by prepare_inputs (size NxK)\n",
    "            y: actual observations (size N)\n",
    "            penalty: the penalty on L2 regularization\n",
    "        Output:\n",
    "            loss: CE loss\n",
    "    \"\"\"\n",
    "    p = forward_fn(params, Z) # N\n",
    "\n",
    "    # Log Probability of each data point, given the model (shape N)\n",
    "    log_probability_of_data = y * jnp.log(p) + (1-y) * jnp.log(1-p)\n",
    "\n",
    "    # We want to maximize the log probability of the data under the model\n",
    "    # but gradient descent minimizes a loss\n",
    "    # So we want to minimize the negative log probability\n",
    "\n",
    "    # compute sum of the weights squared (L2)\n",
    "    sum_weights_squared = jnp.sum(jnp.square(params['W_input_hidden'])) + \\\n",
    "                          jnp.sum(jnp.square(params['W_hidden_output']))\n",
    "\n",
    "    # sum of absolute values of weights (L1)\n",
    "    #sum_weights_squared = jnp.sum(jnp.abs(params['W_input_hidden'])) + \\\n",
    "    #                      jnp.sum(jnp.abs(params['W_hidden_output']))\n",
    "    \n",
    "    # regularize the loss\n",
    "    loss = -jnp.mean(log_probability_of_data) + penalty * sum_weights_squared\n",
    "    \n",
    "    return loss \n",
    "\n",
    "def optimize(rng, input_df, y, learning_rate, epochs, n_hidden, penalty):\n",
    "    \"\"\"\n",
    "        Input parameters:\n",
    "            rng: the random key\n",
    "            input_df: dataframe containing input columns\n",
    "            y: a vector of outputs that we wish to predict\n",
    "            learning_rate: how quickly we want gradient descent learning\n",
    "            epochs: the number of steps of gradient descent\n",
    "            n_hidden: the number of hidden units\n",
    "            penalty: L2 penalty\n",
    "        Output:\n",
    "            params: fitted model parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # To make this work, we need to convert y to jax\n",
    "    y = jnp.array(y)\n",
    "    \n",
    "    # the magic: the gradient function\n",
    "    # Creates a function that can calculate the  gradient of the model\n",
    "    grad_fn = jax.grad(loss_fn) \n",
    "    \n",
    "    # Prepare our inputs into the linear regression\n",
    "    Z = prepare_inputs(input_df) # NxK\n",
    "\n",
    "    #\n",
    "    # Initialize the parameters\n",
    "    #\n",
    "    params = {} # initialize an empty dictionary\n",
    "    # weights from inputs to hidden neurons (dxH)\n",
    "    n_inputs = Z.shape[1]\n",
    "    params['W_input_hidden'] = jax.random.normal(rng, (n_inputs, n_hidden)) / jnp.sqrt(n_inputs)\n",
    "    rng, _ = jax.random.split(rng)  # move to the next random key\n",
    "    \n",
    "    # bias of the hidden neurons is initialized to zero\n",
    "    params['b_hidden'] = jnp.zeros(n_hidden)\n",
    "    \n",
    "    # weights from the hidden neurons to the output neuron (shape is H)\n",
    "    params['W_hidden_output'] = jax.random.normal(rng, (n_hidden,)) / jnp.sqrt(n_hidden)\n",
    "    rng, _ = jax.random.split(rng) # move to next random key\n",
    "    \n",
    "    # finally, initialize the bias of the output neuron \n",
    "    params['b_output'] = 0.0\n",
    "    \n",
    "    # Run gradient descent loop\n",
    "    for i in range(epochs):\n",
    "\n",
    "        # Compute the gradient of the loss function with respect\n",
    "        # to all model parameters\n",
    "        W_grad = grad_fn(params, Z, y, penalty)\n",
    "        \n",
    "        # Update the parameters\n",
    "        for key in params:\n",
    "            params[key] = params[key] - W_grad[key] * learning_rate\n",
    "\n",
    "    # params is the fitted parameter values\n",
    "    return params\n",
    "\n",
    "def mlp_train_test_function(rng,\n",
    "                            train_df, \n",
    "                            test_df, \n",
    "                            input_cols, \n",
    "                            output_col,\n",
    "                            n_hidden,\n",
    "                            penalty,\n",
    "                            n_epochs):\n",
    "    \"\"\"\n",
    "        Function that trains an MLP and tests it. Returns multiple evaluation metrics.\n",
    "\n",
    "        Inputs:\n",
    "            rng: Random number key\n",
    "            train_df: training data frame\n",
    "            test_df: testing data frame \n",
    "            input_cols: features to use\n",
    "            output_col: output to predict\n",
    "            n_hidden: number of hidden units \n",
    "            penalty: L2 penalty\n",
    "            n_epochs: number of epochs to run\n",
    "        Outputs:\n",
    "            results: A dictionary containing accuracy, accuracy_null, auc_roc, auc_pr, auc_pr_null\n",
    "    \"\"\"\n",
    "    # build the training input data frame\n",
    "    train_input_df = train_df[input_cols]\n",
    "\n",
    "    # build the training outputs\n",
    "    y = train_df[output_col].to_numpy()\n",
    "    \n",
    "    # Optimize the model using gradient descent\n",
    "    best_params = optimize(rng = rng,\n",
    "                           input_df = train_input_df,\n",
    "                           y = y,\n",
    "                           learning_rate = 0.1,\n",
    "                           epochs = n_epochs,\n",
    "                           n_hidden = n_hidden,\n",
    "                           penalty = penalty)\n",
    "\n",
    "    # build the testing input data frame\n",
    "    test_input_df = test_df[input_cols]\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    yhat = predict(params = best_params, input_df = test_input_df)\n",
    "    \n",
    "    # Calculate error of those predictions\n",
    "    ytest = test_df[output_col].to_numpy()\n",
    "\n",
    "    # We will return multiple metrics\n",
    "    yhat_hard = yhat > 0.5\n",
    "\n",
    "    # this is our null model, and it is based only on the training set\n",
    "    yhat_null = jnp.mean(y) * jnp.ones_like(ytest)\n",
    "    yhat_hard_null = yhat_null > 0.5\n",
    "    \n",
    "    return dict(\n",
    "        accuracy = sklearn.metrics.accuracy_score(ytest, yhat_hard),\n",
    "        accuracy_null = sklearn.metrics.accuracy_score(ytest, yhat_hard_null),\n",
    "        auc_roc = sklearn.metrics.roc_auc_score(ytest, yhat), # soft decision\n",
    "        auc_pr = sklearn.metrics.average_precision_score(ytest, yhat),\n",
    "        auc_pr_null = sklearn.metrics.average_precision_score(ytest, yhat_null)\n",
    "    )\n",
    "\n",
    "def factory(rng, input_cols, output_col, n_hidden, penalty, n_epochs):\n",
    "\n",
    "    def train_test_fn(train_df, test_df):\n",
    "\n",
    "        return mlp_train_test_function(rng,\n",
    "                                  train_df = train_df,\n",
    "                                  test_df = test_df,\n",
    "                                  input_cols = input_cols,\n",
    "                                  output_col = output_col,\n",
    "                                  n_hidden=n_hidden,\n",
    "                                  penalty=penalty,\n",
    "                                  n_epochs = n_epochs)\n",
    "\n",
    "    return train_test_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c90186a-5776-4573-ab40-f36dad805d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(df, train_test_fn, folds, random_state):\n",
    "    \"\"\"\n",
    "        Cross-validation: splits dataset into N folds, repeatedly trains on N-1 folds and test on the remaining.\n",
    "\n",
    "        Inputs:\n",
    "            df: dataframe of inputs and outputs\n",
    "            train_test_fn: the training and testing function used\n",
    "            folds: number of folds\n",
    "            random_state: pseudo random number generator state\n",
    "        Output:\n",
    "            metrics: loss on each split (size N)\n",
    "    \n",
    "    \"\"\"\n",
    "    # instantiate the splitter\n",
    "    kf = sklearn.model_selection.KFold(n_splits=folds, \n",
    "                                       shuffle=True, \n",
    "                                       random_state=random_state)\n",
    "    \n",
    "    metrics = []\n",
    "    for train_index, test_index in kf.split(df):\n",
    "        train_df = df.iloc[train_index]\n",
    "        test_df = df.iloc[test_index]\n",
    "        \n",
    "        # evaluate\n",
    "        metric = train_test_fn(train_df, test_df)\n",
    "        metrics.append(metric)\n",
    "    \n",
    "    return metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "371dc1fd-8ebd-4ecf-9a34-56bb7d88fb6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy         0.849000\n",
       "accuracy_null    0.481000\n",
       "auc_roc          0.930357\n",
       "auc_pr           0.928479\n",
       "auc_pr_null      0.498000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/overfitting.csv\")\n",
    "\n",
    "# identify the input column names (everything except last column)\n",
    "input_cols = df.columns[:-1]\n",
    "\n",
    "# identify the output column name (last column)\n",
    "output_col = df.columns[-1]\n",
    "\n",
    "# setup the random number seed\n",
    "rng = jax.random.key(42)\n",
    "\n",
    "# setup the training and testing function by calling the factory\n",
    "train_test_fn = factory(rng,\n",
    "                        input_cols,\n",
    "                        output_col,\n",
    "                        n_hidden = 20,\n",
    "                        penalty = 0.01,\n",
    "                        n_epochs = 500) # penalty of 0 means no regularization\n",
    "\n",
    "# run cross validation\n",
    "results = cv(df = df,\n",
    "             train_test_fn = train_test_fn,\n",
    "             folds = 5,\n",
    "             random_state = 234234)\n",
    "# print average results in a nice way\n",
    "pd.DataFrame(results).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741d6a68-a343-43f9-88c9-2b98851432bc",
   "metadata": {},
   "source": [
    "# Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f88a211e-130d-41f4-821e-783a6e479937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the Early stopping variant\n",
      "Stopping at epoch 1174\n",
      "Using the Early stopping variant\n",
      "Stopping at epoch 1800\n",
      "Using the Early stopping variant\n",
      "Stopping at epoch 1004\n",
      "Using the Early stopping variant\n",
      "Stopping at epoch 1400\n",
      "Using the Early stopping variant\n",
      "Stopping at epoch 1283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy         0.869500\n",
       "accuracy_null    0.481000\n",
       "auc_roc          0.945832\n",
       "auc_pr           0.943220\n",
       "auc_pr_null      0.498000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def optimize(rng, input_df, y, learning_rate, epochs, n_hidden, penalty):\n",
    "    \"\"\"\n",
    "        Input parameters:\n",
    "            rng: the random key\n",
    "            input_df: dataframe containing input columns\n",
    "            y: a vector of outputs that we wish to predict\n",
    "            learning_rate: how quickly we want gradient descent learning\n",
    "            epochs: the number of steps of gradient descent\n",
    "            n_hidden: the number of hidden units\n",
    "            penalty: L2 penalty\n",
    "        Output:\n",
    "            params: fitted model parameters\n",
    "    \"\"\"\n",
    "    print(\"Using the Early stopping variant\")\n",
    "    # For demonstration, assume the function got one extra parameter\n",
    "    # which is the early stopping patience which measures\n",
    "    # number of epochs to wait for validation loss to increase before giving up\n",
    "    early_stopping_patience = 20 \n",
    "    # proportion of data to sit aside for validation\n",
    "    validation_prop = 0.2\n",
    "    \n",
    "    # To make this work, we need to convert y to jax\n",
    "    y = jnp.array(y)\n",
    "    \n",
    "    # the magic: the gradient function\n",
    "    # Creates a function that can calculate the  gradient of the model\n",
    "    grad_fn = jax.grad(loss_fn) \n",
    "    \n",
    "    # Prepare our inputs into the linear regression\n",
    "    Z = prepare_inputs(input_df) # NxK\n",
    "\n",
    "    #\n",
    "    # Initialize the parameters\n",
    "    #\n",
    "    params = {} # initialize an empty dictionary\n",
    "    # weights from inputs to hidden neurons (dxH)\n",
    "    n_inputs = Z.shape[1]\n",
    "    params['W_input_hidden'] = jax.random.normal(rng, (n_inputs, n_hidden)) / jnp.sqrt(n_inputs)\n",
    "    rng, _ = jax.random.split(rng)  # move to the next random key\n",
    "    \n",
    "    # bias of the hidden neurons is initialized to zero\n",
    "    params['b_hidden'] = jnp.zeros(n_hidden)\n",
    "    \n",
    "    # weights from the hidden neurons to the output neuron (shape is H)\n",
    "    params['W_hidden_output'] = jax.random.normal(rng, (n_hidden,)) / jnp.sqrt(n_hidden)\n",
    "    rng, _ = jax.random.split(rng) # move to next random key\n",
    "    \n",
    "    # finally, initialize the bias of the output neuron \n",
    "    params['b_output'] = 0.0\n",
    "\n",
    "    #\n",
    "    # Prepare data for early stopping\n",
    "    # Objective: split Z and y so that we have \n",
    "    #  Z_opt: optimization inputs and y_opt: optimization outputs\n",
    "    #  Z_valid: validation inputs and y_valid: validation outputs\n",
    "    #\n",
    "\n",
    "    # randomly permute the input data\n",
    "    idx = jax.random.permutation(rng, y.shape[0])\n",
    "\n",
    "    # figure out the number of samples to be used for optimization\n",
    "    n_opt = int((1-validation_prop) * y.shape[0])\n",
    "\n",
    "    # determine data sample indecies to be used for optimization \n",
    "    # and those for validation\n",
    "    opt_idx = idx[:n_opt] # grab the first n_opt elements of idx\n",
    "    valid_idx = idx[n_opt:] # grab the elements after the first n_opt elements\n",
    "\n",
    "    # finally, split the data\n",
    "    Z_opt = Z[opt_idx, :] \n",
    "    y_opt = y[opt_idx]\n",
    "    Z_valid = Z[valid_idx, :]\n",
    "    y_valid = y[valid_idx]\n",
    "\n",
    "    # \n",
    "    # setup the state variables for early stopping\n",
    "    #\n",
    "\n",
    "    # how many epochs has it been since the last best validation loss?\n",
    "    epochs_since_last_best = 0\n",
    "\n",
    "    # what is the best validation loss so far?\n",
    "    best_valid_loss = jnp.inf \n",
    "\n",
    "    # what were the parameters that correspond to the best validation loss?\n",
    "    best_params = copy.deepcopy(params)\n",
    "    \n",
    "    # Run gradient descent loop\n",
    "    for i in range(epochs):\n",
    "\n",
    "        # Compute the gradient of the loss function with respect\n",
    "        # to all model parameters\n",
    "        W_grad = grad_fn(params, Z_opt, y_opt, penalty)\n",
    "        \n",
    "        # Update the parameters\n",
    "        for key in params:\n",
    "            params[key] = params[key] - W_grad[key] * learning_rate\n",
    "\n",
    "        # Now we track validation loss and decide whether we stop or not\n",
    "        valid_loss = loss_fn(params, Z_valid, y_valid, penalty)\n",
    "\n",
    "        # is it the new best loss?\n",
    "        if valid_loss < best_valid_loss:\n",
    "            # yes it is\n",
    "            best_valid_loss = valid_loss\n",
    "            best_params = copy.deepcopy(params)\n",
    "            epochs_since_last_best = 0\n",
    "        else:\n",
    "            # no it is not\n",
    "            epochs_since_last_best += 1\n",
    "\n",
    "            # have we ran out of patience?\n",
    "            if epochs_since_last_best == early_stopping_patience:\n",
    "                print(f\"Stopping at epoch {i}\")\n",
    "                break\n",
    "                \n",
    "    # params is the fitted parameter values\n",
    "    return best_params\n",
    "\n",
    "df = pd.read_csv(\"../data/overfitting.csv\")\n",
    "\n",
    "# identify the input column names (everything except last column)\n",
    "input_cols = df.columns[:-1]\n",
    "\n",
    "# identify the output column name (last column)\n",
    "output_col = df.columns[-1]\n",
    "\n",
    "# setup the random number seed\n",
    "rng = jax.random.key(42)\n",
    "\n",
    "# setup the training and testing function by calling the factory\n",
    "train_test_fn = factory(rng,\n",
    "                        input_cols,\n",
    "                        output_col,\n",
    "                        n_hidden = 20,\n",
    "                        penalty = 0.01, # early stopping + regularization\n",
    "                        n_epochs = 10_000) # penalty of 0 means no regularization\n",
    "\n",
    "# run cross validation\n",
    "results = cv(df = df,\n",
    "             train_test_fn = train_test_fn,\n",
    "             folds = 5,\n",
    "             random_state = 234234)\n",
    "# print average results in a nice way\n",
    "pd.DataFrame(results).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cd87e7d-bc77-4be3-88cd-c03b8c9baed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "109c0f1a-988b-4f37-bda0-e9e27a7884f7",
   "metadata": {},
   "source": [
    "# Multiclass Clasification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a1e2ec9-7c70-4acb-b1ee-587d07c0e3ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = np.array([0.3, 0.6, 0.1])\n",
    "np.argmax(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c90f1feb-517f-4965-8b82-9a3e9bb97bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 1, 0],\n",
       "       [0, 1, 1]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yactual = np.array([0, 1, 2, 2, 1])\n",
    "ypred =   np.array([0, 1, 2, 1, 0])\n",
    "sklearn.metrics.confusion_matrix(yactual, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a664357-492b-4889-aaad-4c181747e551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(yactual, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61fbe5aa-d33a-4902-9c32-93cb91712e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.balanced_accuracy_score(yactual, jnp.zeros_like(ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639b1741-6b10-42a1-959c-7498ff70f217",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
