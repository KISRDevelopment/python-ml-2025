{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa05c4d0",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron\n",
    "\n",
    "## The Kinds of Functions that an MLP Can Represent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a10456fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5c9372e-d04d-4150-9e3a-c25defc4c13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.contour.QuadContourSet at 0x13bbff770>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAboAAAGyCAYAAACSpAHoAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIZBJREFUeJzt3XuMVeW5x/FnQGcAgZGxKE4ZbmJoKJE21AvaNFCpyh+2/lHSf1rRGqIGTSymLdOmckxjphfTSwhBWi2QkxJobZFU44WjAumpVEUJRYUED8qAIlBgZjNHZ5RZJ+/izAAyt7X3urzv83w/ySod3Mxee613r99+nvWuvaqiKIoEAAClBhW9AgAAZImgAwCoRtABAFQj6AAAqhF0AADVCDoAgGoEHQBANYIOAKAaQQcAUI2gAwColmnQLV++XK644goZOXJkvMycOVOefvrpLJ8SAICzVGX5XZd/+9vfZPDgwXL55ZeLe5rVq1fLL3/5S3n99dfl85//fFZPCwBAPkHXk7q6ujjs7rjjjjyfFgBg1Hl5PdHJkyflz3/+s7S1tcUtzJ60t7fHS5fOzk45evSoXHTRRVJVVZXXqgIAPONqslKpJPX19TJoUMKzblHGduzYEV1wwQXR4MGDo9ra2uipp57q9bFLlixx1SULCwsLC0vU09Lc3Jw4hzJvXXZ0dMi+ffukpaVFHn/8cXn00Udl8+bNMnXq1H4rOvdvxo0bJ89uHSMXDGeCKJCHtcevTvX3vbD/8kSPb3t3ZOLnGP4/5R8fLtzTIVkY+ub7mfxeqz7p7JBNH6yU48ePS21trV+ty+rqapk8eXL8/2fMmCGvvPKK/Pa3v5UVK1ac89iampp4+TQXcsNHEHRAHmo+OT/V3zd42Lnv6b4MGjIk+XPUlH98OO+89I8tQ3ceEBlUnfrvhZR1Giv39HDn3c6s2gDo9dy+KYkef2Jvsk/qzoi3yz+MjdqdTTUHv2Ra0TU2NsrcuXPj9qM7ibhmzRrZtGmTPPvss1k+LYAy/eexa4teBSCsoDt06JDceuut8v7778c9VXfxuAu5r33ta1k+LQAUJm5bwk7QPfbYY1n+egAeo20JXzDDA0CMtiW0IugAFF7NaUHb0k8EHYDC0bZElgg6ALQtoRpBByC4tmUl1VxWaFv6y7/RAsCUctqWlaBtaQ9BBxhH2xLaEXQAUkPbEj7yb8QAMIO2JfJA0AGG0baEBQQdgFTQtoSv/Bs1AEygbYm8EHSAUbQtYQVBB0B92zKrao62ZRgIOgDq25awjaADDKJtCUsIOgAVoW0J3xF0gDFFV3O0LZE3gg4AoBpBB6BstC0RAoIOMIS2JSwi6AAAqhF0AMpC2xKhIOgAI2hbwiqCDoCXfLxTAcLESAKQedtSy50KaFuGiaADDCi6bQkUiaADoGoSCvBpjCYAmaJtiaIRdIBytC1hHUEHYMBoWyJEjCgAmaFtCR8QdIBitC0Bgg7AANG2RKgYVQAyoaVtifARdIBStC3Twfm58BF0AIJvW1LNoS8EHYDUcacC+ISgAxSibZkO2pY6EHQA+kTbEqEj6ACkirYlfEPQAcrQtkwHbUs9CDoAvaJtCQ0IOgCpoW0JHxF0gCK0LYFzEXQAemS5bcn5OV0IOgCpoG0JXxF0gBKhtS25UwHywkgDUEjbshLMtkQSBB2gQNHVHG1L+IygA5A7n9uWTETRx9/RBqAQtC2hDUEHBI62JVBg0DU1NcmVV14pI0aMkIsvvlhuueUW2b17d5ZPCcBzPrctoVOmI27z5s2ycOFC2bp1q2zcuFE+/vhjueGGG6StrS3LpwVQJuttS87P6XRelr/8mWeeOevnVatWxZXdtm3b5Ctf+UqWTw2YQNsSKDjoPq2lpSX+s66ursf/3t7eHi9dWltbc1s3ANmjbYki5DbqOjs75b777pPrrrtOpk2b1us5vdra2u6loaEhr9UDzEvatsy7mmO2JbwPOneubufOnbJ27dpeH9PY2BhXfV1Lc3NzXqsHBKfotiUQilxal/fcc488+eSTsmXLFhk7dmyvj6upqYkXAPr4eqeCLkxE0SvToIuiSO69915Zv369bNq0SSZOnJjl0wFQ2rYEvA06165cs2aNbNiwIb6W7uDBg/Hfu/NvQ4cOzfKpAdVoWwKenKNbvnx5fK5t1qxZcumll3Yv69aty/JpASRg+QarsCHz1iUAXTS2LTk/pxsXtQCBoW0JJEPQAYbRtoQFBB0A021L6EfQAQGhbQkkR9ABRtG2PIWJKPoRdAAGhLYlQkXQAYEIrW3JnQrgC0YiYJD1G6zCFoIOQL9oWyJkBB0QANqW2WAiig1hjEYAqaFtCWsIOgB9om2J0BF0gOdoWwKVYUQChtC2PI3zc3YQdAB6RdsSGhB0gMdoWwKVY1QCRtC2hFUEHYAe0baEFgQd4CnaltlhIoot4YxMAGWjbQnLCDrAQ0VXc7QtoQlBB8BU2xL2MDoB5WhbwjqCDvAMbUsgXQQdgIrQtoTvGKGAYknblnlXc7QtkQeCDkDZqOYQAkYp4JGiz88BGhF0gFK0LYFTCDoAZaFtiVAwUgGFbUvfr50D8kTQAaBtCdUIOgCJ0bZESBitgAdoWwLZIegA42hbQjuCDkAitC0RGkYsUDDalkC2CDrAMNqWsICgAzBgtC0RIkYtoARtS6BnBB1g9EucaVvCCoIOwIDQtkSoGLmAArQtgd4RdEBBaFsC+SDoAPSLtiVCxugFAkfbMpmhOw8UvQrIGUEHFIC2JZAfgg5An2hbInSMYCBgtC2B/hF0QM5CaltSzUEDRjGAzHB+Dj4g6IBA0bYEBoagA4ygbQmrMh3JW7ZskZtvvlnq6+ulqqpKnnjiiSyfDvBekefn8kbbEiaCrq2tTaZPny7Lli3L8mkAc2hbAgN3nmRo7ty58QKgWLQtYRmjGciJpbYlYKaiS6q9vT1eurS2tha6PoCGtmXeX/kF+Mariq6pqUlqa2u7l4aGhqJXCTAnjbYlE1HgE6+CrrGxUVpaWrqX5ubmolcJSAVtS6A4XrUua2pq4gVAz2hbAp4F3YkTJ2TPnj3dP+/du1e2b98udXV1Mm7cuCyfGkAZmG0JjTINuldffVVmz57d/fOiRYviP+fPny+rVq3K8qkBdUK5do7zczAVdLNmzZIoirJ8CsB7RZ2fo20JnEKfAkCMtiW0YmQDAQilbQn4iKADFKJtCZxG0AEZCuX6ubTalkxEgY8IOsBztC2ByhB0gDK0LYGzEXSAccy2hHaMcMDj83O0LYHKEXSAItxgFTgXoxxAKphxCV8RdEAGaFsC/iDoACVoWwI9Y6QDMGPozgNFrwIKQNABKaNtCfiFoAMUoG0J9I7RDhj8bkvAEoIOAKAaQQd4Vs0lPT9H2xLoGyMeAKAaQQekgHNzgL8IOsAjtC2B9DHqgQpRzYXjw2mfjRfYcl7RKwCEjJAL05lhx7el6EfQAZ6EHG3LYmiu8NII8Q9z3j5ZfPAg6IAyUMkhBCGG+Ie9rPP5O/aW/Tv5iAcEGHJJqznAMio6IKCAKxdtS1jG6Ac8CDnuVgBkh6ADAqvkaFsCyRB0QEAhVw7alrCOc3RAwQFH2xLo/5KDT6R8fNQDAqriaFsCyRF0QCAhVw7algBBB6gNOQCncI4O5hUZcEnOz9G2BMpDRQfTNFdxtC2BU3gnwCzNIQfgNIIOJvkQcrQtgXzuZsA5OpjiQ8Dlce0cbUvgNN4NMMOXkAOQLyo6mOBTyGV9g9WiHJtSfdbPo3Z3FLYuwJkIOqgXcsiF3Lb8dPD5ikDWj6CDatZCDslRiepH0EEtDSEXSttSa/ARejoQdFBHQ8ABkFQuLXD8aOYDKbEecr6cnwN8wrsCamgLOdqWQDoIOqigLeQApIdzdAieLyFHwAF+oqJD0LSGHG1LID0EHYKlNeTgBy4t0IOgQ5A0hxzVHCCpXVrgEHQIjuaQq1Tpss54AXAak1EQFM0hl2Yl1xV2XFcH5FTRLVu2TCZMmCBDhgyRq6++Wl5++eU8nhbKEHLlV3hnLoA1mQfdunXrZNGiRbJkyRJ57bXXZPr06XLjjTfKoUOHsn5qKELIpYfQgzWZB92vfvUrWbBggdx+++0ydepUeeSRR2TYsGHyhz/8IeunhhKEXHYIO1iQadB1dHTItm3bZM6cOaefcNCg+OeXXnrpnMe3t7dLa2vrWQtsI+QAeB10R44ckZMnT8oll1xy1t+7nw8ePHjO45uamqS2trZ7aWhoyHL1gAEh5ICweTUlq7GxUVpaWrqX5ubmolcJxqs5Qg4oxofTPhvG5QWf+cxnZPDgwfLBBx+c9ffu5zFjxpzz+JqamngBCLl8z9NxGULPN2Dl21F0yHR0V1dXy4wZM+T555/v/rvOzs7455kzZ2b51AiYxpBzAedjyAEWZP4xzl1a8Pvf/15Wr14tb731ltx9993S1tYWz8IEtIdcKAHH7MveqzqE377M/JtRvvWtb8nhw4flgQceiCegfOELX5BnnnnmnAkqgA8hl6YQAg6woCqKokg85S4vcLMv/76zXoaP4ByCZpoquZADjnN1PeNcXfFf8PxJZ4f81/sr4omKI0eOTPTv+a5LmEfAncbEFGjEiIbpai6NkAvlPBzKx7m6sM/VUdGhUCGHnNZwo6qDNgQdzIVcWlWcZoQdfPPh1EtF3i/v3xJ0MBNyBFwyhN3ZuIA8XAQd1IccAVc+wg4aEHRQG3IEXDoIu9Oo6sJE0EFdyBFw6SPsEDKCDpkLKeDyDrmewsPXr+Mi7E6hqgsPQYfMEHB96y003N8TdkB6CDqkjvNwfRtIUBB2fqOqCwtBh9QQcH2zHg5AUQg6mAu4UEKOqs5vVHXhIOhQEc7D9a+SQCDsgMoRdCgLAdc/CyFgPeyo6sJA0EF1wGkIOZ+rOsd62MF/BB0GhIAbOIsHfcIOPiPooC7gNFRxoVV1lsOO9qX/CDqomUmpvYoj7IDyEHRQEXAaq7hQWQw7qjq/EXQIPuAshVwIVZ3VsIO/CDoEex5Oe6sydNbCjqrOXwSdYSEHnKUqLmTWwg5+IugMCj3grIdcKO1Li2FHVecngs4QAq58Vg7UqBxh5x+CzgANAWe9ivs0qjpg4Ag6xUKfSdmFKk4HS2FHVecXgk4hLQHnEHJ6qjqHsEMRCDpFNAWcQ6tSJ8IOeSPoFCDg0hHiwTfEqs4h7JAngi5wWiaadKGKs4OwQ14IukBpCziHkLNV1TmEHfJA0AWGgEuPlQOs7wg7ZI2gC4TGgHOo4tIRclXnEHbIEkHnOW0TTc5EyOFMhB2yQtB5ioBLn/aDaOhVnUPYIQsEnWc0B5xDFYf+EHZIG0HnEa3n4RyqOCRhKeyQPYLOA1Rx2bB4oNTQvrSGqi57BF2BtAecQ6sS5bJU1RF22SLoCkDAZcfKgdFKVUfYIQ0EXY4sBJxDFYc0EXaoFEGnLOAcQg6aqjqHsEMlCLoMEXDZs3LwA2GH8hF0GbAUcA5VnJ+0VXUOYYdy2BgxOSLksmflQIeeaQvv/sIOlaOiSwkBlw9CDg6VHZIg6CpkLeAcqrhwaGxfdiHsMFAEXSAB50PIUcXBN4QdBoKgS8hiwDmEXLg0V3UOYYf+EHQeB5zlkLNy4EI6LIUdkiPo+kHA5Y8DVvq0V3WWUNUlR9B5FnAOIQckZ6mqI+w8CbqHHnpInnrqKdm+fbtUV1fL8ePHJRSWqziHVuW5BnJQ4Zqn4hF2yDXoOjo6ZN68eTJz5kx57LHHJARUcVRxTrkHj65/52vgWWlfEnbILegefPDB+M9Vq1aJ74oMOMsh59vBKK0Dhvs9voadFYQdvD1H197eHi9dWltbM30+Au4U6yGXxUHC17CzUtU5hB26eDUKmpqapLa2tntpaGjILOAIuVMBZznk3IEhy4MDB57iWQl1x8cPVr5IdMRZvHixVFVV9bns2rWr7JVpbGyUlpaW7qW5uVk0BpwvIVdEwPkQclkH3Kefyzc+7IM8EXZI1Lq8//775bbbbuvzMZMmTSp7ZWpqauIlbUWHWxcfAs6xXsXBHtqYtiUKutGjR8dLKAi4s1meVVnkG9/X83XWEHZ2ZTYZZd++fXL06NH4z5MnT8bX0zmTJ0+W4cOHS9YIubNZreJ8ebP7FnaWJqWcibCzKbOge+CBB2T16tXdP3/xi1+M/3zxxRdl1qxZ6gPOl5CjigPORtjZUxVFUSSecpcXuNmXf99ZL8NH9D0wCbhzWQ05n9/YPlV1jsWqzpdxmpdRHr8fkvjkk4/kv5//j3ii4siRI8O9jq4cPgWc9ZAr+sARwhvatxYm9DtGVRdu0PkWcL6EHFUckrB6rs6hhWlHcEFHwPWOKi4MVHX+IOxsCGYP+3Cxd08IueKE/KYNed21sVTRHjP6ASuIoFt7/GrxkQ8hZ/FrvPL8ZhMLiv7A4gPCTrfgWpc+8CHgHGsB52gKOFqYfqGNqZeNvZoiQq4YWqs4X16TlQN8f6jsdGJ0BxZyVluVQF4IO30IugHgjgOEnIXXSFV3GmGnC+fo+mE14BwCDpZxzk4PG3uxDJarOIeQy5fF1xwCKjsdCLoe+BBwDq1KW3x47VYqmCQIu/DRuvQw5KjiAL/QxgwbQedRwDnWZlQ62t5UoV9bZ/n7L6GTjY8o/SDkikPInYtt4idL4X9MWQvTfEXnQ8jRqgTCQAszTDb2WA8sz6pkwon/it5GVg7m5aCyC4/J0exDwFlsVWr9Gq+ssK38RdiFxVzQ+RByfI0XQkBV1zfCLhxmztH5EHCOtYBzCLmwZ2Gid5yzC4OJPeRDyFmt4kJ9Y/ikyG1o5SBeCSo7/6mu6HwIOIdZlYBuVHZ+U7tnLIccsyr1YZv6j8rOXyqDzoeQo1WJtBW1ba1UKmkg7PykqnXpQ8A5tCoBu2hj+kfN3rAccrQq7aCqCwOVnV9UVHQ+hBxVHACrlZ3vgg46HwLOsXYuziHk7F1bx10NEGoLM9iPG4RcMZhw4gf2QRgsfTA45nELM8ig8yHkrM6qBJAMYVe8oIKOOw4Qcih2nxTdMg8VYVesYEatDwHnWKziCDmgcoRdcYIIuhf2X170KtCqhJeo6sJC2BWDETsAtCoBpIWwyx9B1werVRwhFxaquvAQdvlitPaCC8ABZImwyw9B1wNalQgN+y9MhF0+CLoz0KoEBo72ZToIu+wxUv8fVRxCx/4MF2GXLYLO4LVxDgdFaBjHmlgKu7yZHqW0KqEN+xYhOJZzVWc26GhVAumgqkuPparuWI5hZ3KEUsVBM/Z12Ai79JkKOqutSgBhIezSZSboaFXCkrz3Pe3L9BF26Qn6DuMDwTecAAg57Kx8iDiW4V3KVW9BQg7Ij5UDct6o7CqndmTSqoR1jAc9CLvKqGtdUsUB0Ig2ZvlUbTWqOOBsTErRhcquPGpGpbXLBhxCDrCHsDMYdFavjSPkMFBUdfoQdobO0VkLOMdawA3deaCif//htM+mti6AT6ydsxvxxkf+Bd0777wjP/3pT+WFF16QgwcPSn19vXz729+WH//4x1JdXXlCWws5CwFXaaiV+zsthKEbP3l+t6B7r1iqOuC3zIJu165d0tnZKStWrJDJkyfLzp07ZcGCBdLW1iYPP/xw2b+XWZV6ZBFs5a6HhbCDPpaquuOTq0We9yzobrrppnjpMmnSJNm9e7csX7687KCzVsVpCjlfQs1y2OVd1SEflsIuiHN0LS0tUldX1+t/b29vj5cura2t8Z9t746UQUOGSN6o4vQHHLJD+zI/hF3fctsye/bskaVLl8qdd97Z62Oampqktra2e2loaJCiEHL2Qi609bU8tnAuPlT0LvHRfPHixVJVVdXn4s7PnenAgQNxG3PevHnxebreNDY2xlVf19Lc3Cx54wLwdAIj1NAIdb19RZWRL8Iupdbl/fffL7fddlufj3Hn47q89957Mnv2bLn22mvld7/7XZ//rqamJl6KQsBVhpAAikcbM4WgGz16dLwMhKvkXMjNmDFDVq5cKYMG+bvxCbnKaAo57RNTuNRAP8Iup8koLuRmzZol48ePj2dZHj58uPu/jRkzRnxR9GAIPeQ0BRygCWGXQ9Bt3LgxnoDilrFjx57136IoEh9QxZVPe8BR1UEDwu6UzLaAO4/nAq2npWhMOKmM9pCz9jrzwMG2OCXaxmF/12U5CLjyceDXharOjpLxys7UKyfkymc15Ky+7ixYPtD6oGS4sjMx8mhV2rwmLi2aX3/IYxPJlYyGnfrWJQFXHs0HdxSHSw1QBNUVHSFXHkLO1jYJeawiuZLBDxoqg45WZXloU/aNbQMtSsbCTl3QFR1wIYYcAYc8xy2TUvxQMhR2qkZc0SEXIgIuGbYXNCkZCTsVQUerMjmquPJp3W5UdTZZCLvgZ10ScMloPUgDKF9J+QXlwb4yqrjkCLn0sC0rp/nAGqKS4souyJFWdMCFFnK0KbOhcZuGNraRrpLSsAsu6IoOudBoPBgDyE5JYdgFE3S0KpOhisuHxm3MpBSUlIVdEJNRhv/PIJGaYp47xIADgEqVFE1Q0fEqMkLIwep2p6qDpsqOEaYg5GhTFo/tD/iLoAt8ViUHWGSFqg5aqjpG1xlCCzhCzi/sD2hVCjzsCLpAQw5+0rZvQnpfIFulgMMuiFmXWQrpjaztIAqciZuy+q8U6EzM8NY4RYQcsqBtX4X0PkH2SgF+GDEbdCG9ebUdOC1gn5UnxGrBolJgYWeudUnAAeW9b45NqS56NeCRUkBtzDDWMiWEHPLEPoR2pUAqOxNBx7VxKAr7MrlQqgSEE3bqR1RoAceBEb4K6b2EfJU8DzvVQRfSG5OA04t9mxxVXXhKHoedyskoBByQDSalIMQJKv6tUYUIOfiIfZ2cjwdMhEnVSCLk4DMt+zyk9xny52MLU0XrMqQ3npaDHQCE0sL0Z03KRMghJFrGQF7vO58Olgi3sgt6FBFyCBFjAVaUPAm7IIMupAvAuTYOWlHVIZSwC24EhRJwDgGH3jA2YEmp4LALJuhCquIcDmSwMEao6hBC2AUx6/LCPR0i5w2REGg4eAGAptmYfExKESGHpBgzsKZUQGVH0KWACSeoROhjh/YlfA87Ro7xgxQAaA87gq4ChBzSEvpYoqqDz2HHqCkDrUpkgTEFi0o5hB1BlxAHI6BnVHXwFSMmAUIOWWOMwaJSxlUdQTcAtCqBgQnpSx1gJ+wIun4QcMgbY65/tC91KmUUdoyWPnDAQVFCHntUdfAt7Ai6HtCqBPxnsaobPrElXrQrpRx29kZKPwg4+CLksUhVly3CLhmCTsmBBToxJvtmsaqzVN2VUgo7u6PkDLQqgfRR1eWDsOuf+aAj4OA7xij6Q9gVGHRf//rXZdy4cTJkyBC59NJL5Tvf+Y6899574gOqOISEsdo7S+3LE3trzYbdiUnlh12mI2T27Nnypz/9SXbv3i1/+ctf5O2335ZvfvObUjQOGkA+aF/mS3vYeXmH8e9973vd/3/8+PGyePFiueWWW+Tjjz+W888/X4pAyCFUbux+OO2zRa+Gt1VdETf09DXs+qr8LMo06M509OhR+eMf/yjXXnttryHX3t4eL11aWk59Ovnkk48qfv6hb75/6ndV/JuA4py/Y698OPVSCcmINz6S45OrM3+ezo9sBN3J/z19jOzN0EsOSdu7I0WTzo9O5UAURcn/cZSxH/zgB9GwYcPcmkXXXHNNdOTIkV4fu2TJkvhxLCwsLCws0sPy9ttvJ86hKvc/SYLRtR9//vOf9/mYt956Sz73uc/F///IkSNxNffuu+/Kgw8+KLW1tfLkk09KVVVVvxXd8ePH45bnvn374n9nUWtrqzQ0NEhzc7OMHKnrE9pAWH/9DtuAbWD99Xd1+NzkxmPHjsmFF14oSSQOusOHD8u///3vPh8zadIkqa4+t1Wxf//+eGf94x//kJkzZw5o57qAcy/Q6s61vg2sv36HbcA2sP76K90Gic/RjR49Ol7K0dl5qod+ZtUGAECQk1H++c9/yiuvvCJf/vKXZdSoUfGlBT/5yU/ksssuG1A1BwBAGjK7jm7YsGHy17/+Va6//nqZMmWK3HHHHXLFFVfI5s2bpaamZkC/wz1uyZIlA368Rta3gfXX77AN2AbWX3+l2yDxOToAAEJi57tzAAAmEXQAANUIOgCAagQdAEC1oILO59v+ZO2dd96JZ65OnDhRhg4dGl+m4WYgdXTY+nb4hx56KP6+VDerN+m3I4Ro2bJlMmHChHjMX3311fLyyy+LJVu2bJGbb75Z6uvr429TeuKJJ8SSpqYmufLKK2XEiBFy8cUXx1+K7+4GY8Xy5cvj2fruAnG3uEvTnn76ad1B5+ttf/Kwa9eu+IL7FStWyBtvvCG//vWv5ZFHHpEf/ehHYokL9nnz5sndd98t2q1bt04WLVoUf6B57bXXZPr06XLjjTfKoUOHxIq2trb4dbvAt8hdjrVw4ULZunWrbNy4Mb7zyw033BBvFwvGjh0rP/vZz2Tbtm3y6quvyle/+lX5xje+ER8DE4kCtmHDhqiqqirq6OiILPrFL34RTZw4MbJo5cqVUW1tbaTZVVddFS1cuLD755MnT0b19fVRU1NTZJE7XK1fvz6y7NChQ/F22Lx5c2TVqFGjokcffTTRvwmqokt62x/t3He+1dXVFb0ayKhydZ9i58yZ0/13gwYNin9+6aWXCl03FKfr1mUW3/cnT56UtWvXxtVs0m/XCi7ofvjDH8oFF1wgF110UXxXgw0bNohFe/bskaVLl8qdd95Z9KogA+6uH+6Nfckll5z19+7ngwcPFrZeKI47dXHffffJddddJ9OmTRMr/vWvf8nw4cPjb0S56667ZP369TJ16tSwgs7d9sedZO5rceenunz/+9+X119/XZ577jkZPHiw3HrrreXdiM8TSV+/c+DAAbnpppvic1ULFiyQ0JWzDQBr3Lm6nTt3xlWNJVOmTJHt27fH35/szs3Pnz9f3nzzzbC+AizP2/74KOnrd7NMZ82aJddcc42sWrUqbmeFrpwx4F67+3Tr7lmotXXpZpY+/vjj8Uy7Lu5N7l6zxU6G+8DjPs2fuT2suOeee+J97mahupnXls2ZMyeede4m5hV+94KBsn7bnySv31VybubpjBkzZOXKlSpCrtIxoJULdbefn3/++e4Duxvv7md30IMNrg65995744DftGmT+ZDreh8kPeYXHnQDZf22Py7kXCXn7rj+8MMPx1VQlzFjxogV7rysm4jk/nTnsFxLw5k8eXLcx9fEXVrgKrgvfelLctVVV8lvfvOb+ET87bffLlacOHEiPh/dZe/evfE+d5Mx3DW1FtqVa9asias5dy1d1/lZdwNSdz2tdo2NjTJ37tx4X5dKpXhbuMB/9tlnk/2iKBA7duyIZs+eHdXV1UU1NTXRhAkTorvuuivav39/ZGU6vdtdPS2WzJ8/v8dt8OKLL0YaLV26NBo3blxUXV0dX26wdevWyBK3X3va324cWNDbe94dDyz47ne/G40fPz4e/6NHj46uv/766Lnnnkv8ewo/RwcAQJZ0nOQBAKAXBB0AQDWCDgCgGkEHAFCNoAMAqEbQAQBUI+gAAKoRdAAA1Qg6AIBqBB0AQDWCDgCgGkEHABDN/g+xLP9FK25S1AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Create a two dimensional grid that we want to makes predictions over\n",
    "#\n",
    "\n",
    "# The number of points in each dimension\n",
    "N = 100\n",
    "xs = jnp.linspace(-3, 3, N)\n",
    "x1s, x2s = jnp.meshgrid(xs, xs) # creates all combinations of xs * xs\n",
    "# so now x1s and x2s have shape 100x100\n",
    "\n",
    "# Now we make the actual inputs into the neural network\n",
    "# x1s.flatten() gives us a shape of 100*100 = 10,000\n",
    "# and similary for x2s\n",
    "X = jnp.vstack((x1s.flatten(), x2s.flatten())).T\n",
    "\n",
    "# First, let's prepare a random key\n",
    "rng = jax.random.key(123413)\n",
    "n_hidden = 20 # number of hidden units in the first hidden layer\n",
    "\n",
    "#\n",
    "# Now let's initialize the weights of the neural network\n",
    "#\n",
    "params = {} # initialize an empty dictionary\n",
    "\n",
    "# weights from inputs to hidden neurons (2xH)\n",
    "params['W_input_hidden'] = jax.random.normal(rng, (2, n_hidden)) \n",
    "rng, _ = jax.random.split(rng)  # move to the next random key\n",
    "\n",
    "# bias of the hidden neurons is initialized to zero\n",
    "params['b_hidden'] = jnp.zeros(n_hidden)\n",
    "\n",
    "# weights from the hidden neurons to the output neuron (shape is H)\n",
    "params['W_hidden_output'] = jax.random.normal(rng, (n_hidden,)) \n",
    "rng, _ = jax.random.split(rng) # move to next random key\n",
    "\n",
    "# finally, initialize the bias of the output neuron \n",
    "params['b_output'] = 0.0\n",
    "\n",
    "# \n",
    "# Now we can actually run the forward pass of the network\n",
    "#\n",
    "\n",
    "# compute the inputs into the neurons\n",
    "# 1xH + Nx2 @ 2xH = 1xH + NxH = NxH\n",
    "hidden_inputs = params['b_hidden'][None,:] + X @ params['W_input_hidden']\n",
    "\n",
    "# next, we apply the non-linearity to those inputs\n",
    "#hidden_activations = jnp.tanh(hidden_inputs) # NxH\n",
    "hidden_activations = jnp.maximum(0, hidden_inputs) # <-- ReLU\n",
    "\n",
    "# now we compute the output\n",
    "# 1 + NxH @ H = 1 + N = N \n",
    "f = params['b_output'] + hidden_activations @ params['W_hidden_output']\n",
    "p = 1/(1+jnp.exp(-f)) # <-- sigmoid function\n",
    "\n",
    "# Now we want to plot the result\n",
    "\n",
    "# first, we need to reshape the output from N (10,000) to 100x100 \n",
    "P = jnp.reshape(p, (N, N)) # 10,000 -> 100x100\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=(5,5))\n",
    "ax.contourf(x1s, x2s, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e7c221-53bc-40dc-a10f-9366b95a2d83",
   "metadata": {},
   "source": [
    "# Implementing the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b80da8ab-b0d2-4b9d-bb68-53323ada60ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.894"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_inputs(input_df):\n",
    "    \"\"\"\n",
    "        Prepares the input features that will be fed into the model.\n",
    "\n",
    "        Inputs:\n",
    "            input_df: the input dataframe into the function. Should consist ONLY of input features.\n",
    "        Outputs:\n",
    "            Z: the input feature matrix of size NxK, where K is the number of features\n",
    "    \"\"\"\n",
    "    # Let's identify categorical columns in a dataframe\n",
    "    categorical_cols = input_df.select_dtypes(include='object').columns\n",
    "    \n",
    "    # Let's identify the numeric columns in the dataframe\n",
    "    numeric_cols = input_df.select_dtypes(include='number').columns\n",
    "\n",
    "    # We want to construct the input features into the model\n",
    "    # We will use a numpy array that contains both numeric and categorically encoded values\n",
    "    X = input_df[numeric_cols].to_numpy() # (NxK)\n",
    "    \n",
    "    # Now we need to z-score the numeric features so that they can lead to efficient learning\n",
    "    col_means = np.mean(X, axis=0) # K\n",
    "    col_stds = np.std(X, axis=0, ddof=1) # K\n",
    "    \n",
    "    # Z-score\n",
    "    # (NxK - \n",
    "    #  1xK) \n",
    "    #  / \n",
    "    #  (1xK)\n",
    "    Z = (X - col_means[None, :]) / col_stds[None, :]\n",
    "    \n",
    "    # Now we want to code the categorical columns using one-hot encoding\n",
    "    for col in categorical_cols:\n",
    "        # NxC (C is the number of unique values in the column)\n",
    "        # So for origin this will be Nx3 \n",
    "        dummies = pd.get_dummies( input_df[col] ).to_numpy() \n",
    "        \n",
    "        # concatenate dummies matrix onto Z\n",
    "        #print(Z.shape)\n",
    "        #print(dummies.shape)\n",
    "        Z = np.hstack((Z, dummies)) \n",
    "    \n",
    "    # finally we want to add a column of ones at the start of Z\n",
    "    ones_col = np.ones((Z.shape[0], 1)) # Nx1\n",
    "    \n",
    "    Z = np.hstack((ones_col, Z))\n",
    "\n",
    "    return Z\n",
    "\n",
    "def forward_fn(params, Z):\n",
    "    \"\"\"\n",
    "        The MLP forward function.\n",
    "        This is a neuron network with one hidden layer and ReLU activations.\n",
    "        \n",
    "        Inputs:\n",
    "            params: the weight of the model\n",
    "            Z: the input feature matrix, as returned by prepare_inputs (size NxK)\n",
    "        Output:\n",
    "            yhat: the model's predictions (size N)\n",
    "    \"\"\"\n",
    "    # compute the inputs into the neurons\n",
    "    # 1xH + Nxd @ dxH = 1xH + NxH = NxH\n",
    "    hidden_inputs = params['b_hidden'][None,:] + Z @ params['W_input_hidden']\n",
    "    \n",
    "    # next, we apply the non-linearity to those inputs\n",
    "    #hidden_activations = jnp.tanh(hidden_inputs) # NxH\n",
    "    hidden_activations = jnp.maximum(0, hidden_inputs) # <-- ReLU\n",
    "    \n",
    "    # now we compute the output\n",
    "    # 1 + NxH @ H = 1 + N = N \n",
    "    f = params['b_output'] + hidden_activations @ params['W_hidden_output']\n",
    "    p = 1/(1+jnp.exp(-f)) # <-- sigmoid function (the probability of a positive)\n",
    "\n",
    "    # so that the log doesn't blow up\n",
    "    p = jnp.clip(p, 0.01, 0.99)\n",
    "    jnp.log_simoid\n",
    "    return p\n",
    "\n",
    "def predict(params, input_df):\n",
    "    \"\"\"\n",
    "        Convienience function that prepares inputs and runs the forward function.\n",
    "\n",
    "        Inputs:\n",
    "            params: the weights of the model\n",
    "            input_df: input data frame (input features only, no output column).\n",
    "        Output:\n",
    "            yhat: the model's predictions (size N)\n",
    "    \"\"\"\n",
    "    Z = prepare_inputs(input_df)\n",
    "    return forward_fn(params, Z)\n",
    "\n",
    "def loss_fn(params, Z, y):\n",
    "    \"\"\"\n",
    "        Computes the mean squared error loss function for the model.\n",
    "\n",
    "        Inputs:\n",
    "            params: the weights of the model\n",
    "            Z: the input feature matrix, as returned by prepare_inputs (size NxK)\n",
    "            y: actual observations (size N)\n",
    "        Output:\n",
    "            mse: mean squared error\n",
    "    \"\"\"\n",
    "    p = forward_fn(params, Z) # N\n",
    "\n",
    "    # Log Probability of each data point, given the model (shape N)\n",
    "    log_probability_of_data = y * jnp.log(p) + (1-y) * jnp.log(1-p)\n",
    "\n",
    "    # We want to maximize the log probability of the data under the model\n",
    "    # but gradient descent minimizes a loss\n",
    "    # So we want to minimize the negative log probability\n",
    "    loss = -jnp.mean(log_probability_of_data)\n",
    "    \n",
    "    return loss \n",
    "\n",
    "def optimize(rng, input_df, y, learning_rate, epochs, n_hidden):\n",
    "    \"\"\"\n",
    "        Input parameters:\n",
    "            rng: the random key\n",
    "            input_df: dataframe containing input columns\n",
    "            y: a vector of outputs that we wish to predict\n",
    "            learning_rate: how quickly we want gradient descent learning\n",
    "            epochs: the number of steps of gradient descent\n",
    "            n_hidden: the number of hidden units\n",
    "        Output:\n",
    "            params: fitted model parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # To make this work, we need to convert y to jax\n",
    "    y = jnp.array(y)\n",
    "    \n",
    "    # the magic: the gradient function\n",
    "    # Creates a function that can calculate the  gradient of the model\n",
    "    grad_fn = jax.grad(loss_fn) \n",
    "    \n",
    "    # Prepare our inputs into the linear regression\n",
    "    Z = prepare_inputs(input_df) # NxK\n",
    "\n",
    "    #\n",
    "    # Initialize the parameters\n",
    "    #\n",
    "    params = {} # initialize an empty dictionary\n",
    "    # weights from inputs to hidden neurons (dxH)\n",
    "    n_inputs = Z.shape[1]\n",
    "    params['W_input_hidden'] = jax.random.normal(rng, (n_inputs, n_hidden)) / jnp.sqrt(n_inputs)\n",
    "    rng, _ = jax.random.split(rng)  # move to the next random key\n",
    "    \n",
    "    # bias of the hidden neurons is initialized to zero\n",
    "    params['b_hidden'] = jnp.zeros(n_hidden)\n",
    "    \n",
    "    # weights from the hidden neurons to the output neuron (shape is H)\n",
    "    params['W_hidden_output'] = jax.random.normal(rng, (n_hidden,)) / jnp.sqrt(n_hidden)\n",
    "    rng, _ = jax.random.split(rng) # move to next random key\n",
    "    \n",
    "    # finally, initialize the bias of the output neuron \n",
    "    params['b_output'] = 0.0\n",
    "    \n",
    "    # Run gradient descent loop\n",
    "    for i in range(epochs):\n",
    "\n",
    "        # Compute the gradient of the loss function with respect\n",
    "        # to all model parameters\n",
    "        W_grad = grad_fn(params, Z, y)\n",
    "        \n",
    "        # Update the parameters\n",
    "        for key in params:\n",
    "            params[key] = params[key] - W_grad[key] * learning_rate\n",
    "\n",
    "    # params is the fitted parameter values\n",
    "    return params\n",
    "\n",
    "def mlp_train_test_function(rng,\n",
    "                            train_df, \n",
    "                            test_df, \n",
    "                            input_cols, \n",
    "                            output_col,\n",
    "                            n_hidden):\n",
    "\n",
    "    # build the training input data frame\n",
    "    train_input_df = train_df[input_cols]\n",
    "\n",
    "    # build the training outputs\n",
    "    y = train_df[output_col].to_numpy()\n",
    "    \n",
    "    # Optimize the model using gradient descent\n",
    "    best_params = optimize(rng = rng,\n",
    "                           input_df = train_input_df,\n",
    "                           y = y,\n",
    "                           learning_rate = 0.1,\n",
    "                           epochs = 100,\n",
    "                           n_hidden = n_hidden)\n",
    "\n",
    "    # build the testing input data frame\n",
    "    test_input_df = test_df[input_cols]\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    yhat = predict(params = best_params, input_df = test_input_df)\n",
    "    \n",
    "    # Calculate error of those predictions\n",
    "    ytest = test_df[output_col].to_numpy()\n",
    "    \n",
    "    return sklearn.metrics.accuracy_score(ytest, yhat > 0.5)\n",
    "# load the example 'hard' dataset\n",
    "df = pd.read_csv(\"../data/nonseparable_binary_data.csv\")\n",
    "# initialize random number generator\n",
    "rng = jax.random.key(5635636)\n",
    "# compute accuracy on the training set, using an MLP with 2 hidden units\n",
    "accuracy = mlp_train_test_function(rng,\n",
    "                                   train_df = df,\n",
    "                                   test_df = df,\n",
    "                                   input_cols = ['x1','x2'],\n",
    "                                   output_col = 'y',\n",
    "                                   n_hidden=20)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac8ac9-c481-4626-b2b5-b94dfe8ab832",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9da731cd-5d72-43aa-9e4f-784cccf8f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a toy dataset\n",
    "# where we have some actual observations and corresponding predictions\n",
    "y_actual = np.array([0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0])\n",
    "y_pred = np.array([0.06, 0.92, 0.86, 0.03, 0.4, 0.7, \n",
    "                   0.23, 0.4, .2, 0.8, 0.9, 0.65, 0.75, 0.4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1bcdd71d-db6f-47aa-ad46-61eb171fb0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7142857142857143"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we want to see the metrics that require hard decisions\n",
    "y_pred_hard = y_pred > 0.5\n",
    "\n",
    "sklearn.metrics.accuracy_score(y_actual, y_pred_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5580e6ad-c3d9-41f7-b519-77ab49fa73d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones_like(y_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e4f3fc65-2713-4f66-a246-fccb5a967302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6428571428571429"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's also define our null model. \n",
    "# This will be a model that always predicts the proportion of positives on the dataset\n",
    "y_pred_null = np.ones_like(y_actual) * np.mean(y_actual)\n",
    "y_pred_null_hard = y_pred_null > 0.5\n",
    "\n",
    "sklearn.metrics.accuracy_score(y_actual, y_pred_null_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "10dd7030-a953-4718-b595-20e657a94dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7333333333333334"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.balanced_accuracy_score(y_actual, y_pred_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "758619f5-be6c-4c8e-afc1-a68f06185d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.balanced_accuracy_score(y_actual, y_pred_null_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "953a25b6-2caf-43b8-9680-a1858c57e712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.recall_score(y_actual, y_pred_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3e6fb315-1fa5-4911-a305-2d09627da240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.recall_score(y_actual, y_pred_null_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0e0fa019-fcea-4be8-851b-8f769ed4941f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.precision_score(y_actual, y_pred_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8f137c27-c307-4d04-ae2b-bc72dc86909e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6428571428571429"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.precision_score(y_actual, y_pred_null_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4f812f1e-e2f8-4435-9310-e59169155d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.f1_score(y_actual, y_pred_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "93453e51-31ab-4171-bc89-1ba1360d84c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.782608695652174"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.f1_score(y_actual, y_pred_null_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4c1d30a9-efdd-49c6-8c05-0812e8b18e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7555555555555555"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the metrics that are based on soft predictions\n",
    "\n",
    "sklearn.metrics.roc_auc_score(y_actual, y_pred) # Not the hard predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "10df573f-23dd-4db3-94bc-78c2638f40e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.roc_auc_score(y_actual, y_pred_null) # Not the hard predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "547fdb34-71f0-4320-b33a-de13a96a259d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8063492063492064"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AUC-PR\n",
    "sklearn.metrics.average_precision_score(y_actual, y_pred) # Not the hard predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d713789e-5b9e-4fd5-b1ab-8c59bba7b74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6428571428571429"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.average_precision_score(y_actual, y_pred_null) # Not the hard predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597b7708-82e7-410d-a52d-4387b8196ecd",
   "metadata": {},
   "source": [
    "# Let's try to do CV on our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1ddf3215-6557-4dda-885b-d811a0995c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy         0.912842\n",
       "accuracy_null    0.605956\n",
       "auc_roc          0.958677\n",
       "auc_pr           0.934843\n",
       "auc_pr_null      0.394044\n",
       "dtype: float64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cv(df, train_test_fn, folds, random_state):\n",
    "    \"\"\"\n",
    "        Cross-validation: splits dataset into N folds, repeatedly trains on N-1 folds and test on the remaining.\n",
    "\n",
    "        Inputs:\n",
    "            df: dataframe of inputs and outputs\n",
    "            train_test_fn: the training and testing function used\n",
    "            folds: number of folds\n",
    "            random_state: pseudo random number generator state\n",
    "        Output:\n",
    "            metrics: loss on each split (size N)\n",
    "    \n",
    "    \"\"\"\n",
    "    # instantiate the splitter\n",
    "    kf = sklearn.model_selection.KFold(n_splits=folds, \n",
    "                                       shuffle=True, \n",
    "                                       random_state=random_state)\n",
    "    \n",
    "    metrics = []\n",
    "    for train_index, test_index in kf.split(df):\n",
    "        train_df = df.iloc[train_index]\n",
    "        test_df = df.iloc[test_index]\n",
    "        \n",
    "        # evaluate\n",
    "        metric = train_test_fn(train_df, test_df)\n",
    "        metrics.append(metric)\n",
    "    \n",
    "    return metrics \n",
    "\n",
    "def mlp_train_test_function(rng,\n",
    "                            train_df, \n",
    "                            test_df, \n",
    "                            input_cols, \n",
    "                            output_col,\n",
    "                            n_hidden):\n",
    "\n",
    "    # build the training input data frame\n",
    "    train_input_df = train_df[input_cols]\n",
    "\n",
    "    # build the training outputs\n",
    "    y = train_df[output_col].to_numpy()\n",
    "    \n",
    "    # Optimize the model using gradient descent\n",
    "    best_params = optimize(rng = rng,\n",
    "                           input_df = train_input_df,\n",
    "                           y = y,\n",
    "                           learning_rate = 0.1,\n",
    "                           epochs = 100,\n",
    "                           n_hidden = n_hidden)\n",
    "\n",
    "    # build the testing input data frame\n",
    "    test_input_df = test_df[input_cols]\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    yhat = predict(params = best_params, input_df = test_input_df)\n",
    "    \n",
    "    # Calculate error of those predictions\n",
    "    ytest = test_df[output_col].to_numpy()\n",
    "\n",
    "    # We will return multiple metrics\n",
    "    yhat_hard = yhat > 0.5\n",
    "\n",
    "    # this is our null model, and it is based only on the training set\n",
    "    yhat_null = jnp.mean(y) * jnp.ones_like(ytest)\n",
    "    yhat_hard_null = yhat_null > 0.5\n",
    "    \n",
    "    return dict(\n",
    "        accuracy = sklearn.metrics.accuracy_score(ytest, yhat_hard),\n",
    "        accuracy_null = sklearn.metrics.accuracy_score(ytest, yhat_hard_null),\n",
    "        auc_roc = sklearn.metrics.roc_auc_score(ytest, yhat), # soft decision\n",
    "        auc_pr = sklearn.metrics.average_precision_score(ytest, yhat),\n",
    "        auc_pr_null = sklearn.metrics.average_precision_score(ytest, yhat_null)\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(\"../data/nonseparable_binary_data.csv\")\n",
    "\n",
    "# get the metrics on the training set ... just to make sure that the function\n",
    "# works\n",
    "metrics = mlp_train_test_function(rng,\n",
    "                                  train_df = df,\n",
    "                                  test_df = df,\n",
    "                                  input_cols = ['x1','x2'],\n",
    "                                  output_col = 'y',\n",
    "                                  n_hidden=10)\n",
    "\n",
    "def factory(rng, input_cols, output_col, n_hidden):\n",
    "\n",
    "    def train_test_fn(train_df, test_df):\n",
    "\n",
    "        return mlp_train_test_function(rng,\n",
    "                                  train_df = train_df,\n",
    "                                  test_df = test_df,\n",
    "                                  input_cols = input_cols,\n",
    "                                  output_col = output_col,\n",
    "                                  n_hidden=n_hidden)\n",
    "\n",
    "    return train_test_fn\n",
    "\n",
    "# Load the spambase dataset\n",
    "df = pd.read_csv(\"../data/spambase.csv\")\n",
    "\n",
    "# grab all column names, except the last one\n",
    "input_cols = df.columns[:-1]\n",
    "\n",
    "# grab the output column name\n",
    "output_col = df.columns[-1]\n",
    "\n",
    "rng = jax.random.key(4234)\n",
    "train_test_fn = factory(rng,\n",
    "                        input_cols = input_cols,\n",
    "                        output_col = output_col,\n",
    "                        n_hidden = 2)\n",
    "results = cv(df = df,\n",
    "             train_test_fn = train_test_fn,\n",
    "             folds = 5,\n",
    "             random_state = 234234)\n",
    "results_df = pd.DataFrame(results)\n",
    "np.mean(results_df, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "314ff87a-6831-4f33-ac21-c6b0737c6440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['a','b','c','d','e']\n",
    "a[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b342da9b-a164-49b8-b369-f5ea91b2ec07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
