{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5959cb1-2600-4b20-b9c5-302856ca5743",
   "metadata": {},
   "source": [
    "# Day 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a7e312-c1eb-44bb-92b9-11ca9bfaae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import linear_model_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7028aa7a-c8f3-4609-86da-d26852523151",
   "metadata": {},
   "source": [
    "## Functions as First Class Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcb0b9a8-71e5-4261-ac02-eb68f7febbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider a simple function that takes two numbers and adds them\n",
    "def myfunc(a, b):\n",
    "    return a + b\n",
    "myfunc(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdac7693-ca11-4bc7-84df-f38111a9d80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can make a function that returns a function\n",
    "def wrapper():\n",
    "    # wrapper doesn't take any arguments\n",
    "    def wrapped(a, b):\n",
    "        return a + b\n",
    "    return wrapped # wrapper returns wrapped\n",
    "\n",
    "# lets call wrapper\n",
    "myfunc = wrapper() # now myfunc is whatever wrapper returned\n",
    "myfunc(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbb12a98-f6ea-4f02-af60-598d4fa7aa38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but we can pass some arguments to the wrapper\n",
    "def wrapper(a):\n",
    "    def wrapped(b):\n",
    "        return a + b\n",
    "    return wrapped\n",
    "\n",
    "myfunc = wrapper(5)\n",
    "myfunc(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f06d07e6-4b00-43b4-8a3c-57cea25c3e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to make a version of the function mvlr_train_test_function \n",
    "# that takes train_df and test_df ONLY\n",
    "def factory(input_cols, output_col):\n",
    "    def train_test_fn(train_df, test_df):\n",
    "        return linear_model_numpy.mvlr_train_test_function(train_df,\n",
    "                                                           test_df,\n",
    "                                                           input_cols,\n",
    "                                                           output_col)\n",
    "    return train_test_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9cad1c3-ad52-4576-8b61-2316b8a8d6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:  (406, 9)\n",
      "After cleaning:  (392, 9)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json('../data/cars.json')\n",
    "\n",
    "#\n",
    "# We want to do: Exclude ANY row that has ANY missing value\n",
    "#\n",
    "\n",
    "# find everywhere where there are missing values\n",
    "missing_vals = pd.isna(df)\n",
    "\n",
    "# we want to go over every row and find how many missing missing values there are\n",
    "num_missing = np.sum(missing_vals, axis=1) # this will take NxK => N\n",
    "\n",
    "# we want an array that tells us which rows should be included\n",
    "ix = (num_missing == 0)\n",
    "np.sum(ix)\n",
    "\n",
    "# use ix to index into the dataframe and make it clean\n",
    "print(\"Before cleaning: \", df.shape)\n",
    "df = df[ix]\n",
    "print(\"After cleaning: \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be72c329-d52c-47f5-b5cf-7b26419234c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(24.391855009262596),\n",
       " np.float64(23.039246113140706),\n",
       " np.float64(13.333147700048498),\n",
       " np.float64(21.440681627535316),\n",
       " np.float64(25.82010852738354)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's do a quick cross-validation run to test our little factory\n",
    "\n",
    "# brings in the KFold class which does CV splits\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cv(df, train_test_fn, folds, random_state):\n",
    "    \n",
    "    # initialize the splitter\n",
    "    splitter = KFold(n_splits = folds,\n",
    "                     shuffle = True,\n",
    "                     random_state = random_state)\n",
    "    \n",
    "    # run the CV loop\n",
    "    metrics = [] # an array that will hold each iteration's result\n",
    "    \n",
    "    for train_indecies, test_indecies in splitter.split(df):\n",
    "        \n",
    "        # construct the training and test dataframes\n",
    "        train_df = df.iloc[train_indecies]\n",
    "        test_df = df.iloc[test_indecies]\n",
    "    \n",
    "        # evaluate\n",
    "        mse = train_test_fn(train_df, test_df)\n",
    "        metrics.append(mse)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# lets create a train test function from the factory\n",
    "train_test_fn = factory(input_cols = ['Cylinders', 'Displacement'],\n",
    "                        output_col = 'Miles_per_Gallon')\n",
    "\n",
    "cv(df = df, \n",
    "   train_test_fn = train_test_fn,\n",
    "   folds = 5,\n",
    "   random_state = 123123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e959547-7e87-4713-a4ea-8c85667b11ec",
   "metadata": {},
   "source": [
    "# Brute Force Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8625158b-278c-45d5-aa98-c202aa9b4edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a',), ('b',), ('c',), ('a', 'b'), ('a', 'c'), ('b', 'c'), ('a', 'b', 'c')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# suppose you have three features\n",
    "my_features = ['a', 'b', 'c']\n",
    "\n",
    "# we want all possible combinations of these three features\n",
    "# ['a'], ['b'], ['c'], ['a', 'b'], ['a', 'c'], ... \n",
    "\n",
    "# list combinations of length 1\n",
    "list(itertools.combinations(my_features, 1))\n",
    "list(itertools.combinations(my_features, 2)) # length 2\n",
    "list(itertools.combinations(my_features, 3)) # length 3\n",
    "\n",
    "# let make a list with all combinations concatenated\n",
    "all_combinations = [\n",
    "    combination\n",
    "    for combination_length in range(1, len(my_features)+1)\n",
    "    for combination in itertools.combinations(my_features, combination_length)\n",
    "]\n",
    "\n",
    "# start with empty list\n",
    "all_combinations = []\n",
    "for combination_length in range(1, len(my_features)+1):\n",
    "    for combination in itertools.combinations(my_features, combination_length):\n",
    "        all_combinations.append(combination)\n",
    "\n",
    "# print it out\n",
    "all_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3065f33f-5b3e-47ea-add5-dfa6a17023f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Weight_in_lbs', 'Origin', 'Year']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def brute_force_selection(df, # dataset\n",
    "                          input_cols, # list of features to search over\n",
    "                          output_col, # output column\n",
    "                          factory): # factory\n",
    "\n",
    "    # generate all possible input feature sets\n",
    "    all_combinations = [\n",
    "        list(combination) # we have to convert from a tuple to a list, \n",
    "        # otherwise we can't index into pandas dataframes\n",
    "        for combination_length in range(1, len(input_cols)+1)\n",
    "        for combination in itertools.combinations(input_cols, combination_length)\n",
    "    ]\n",
    "\n",
    "    # we want to go over every combination and test it\n",
    "    # so we need to track the performance of each combination\n",
    "    all_metrics = []\n",
    "    for combination in all_combinations:\n",
    "\n",
    "        # call our factory to get the train function\n",
    "        train_test_fn = factory(input_cols = combination,\n",
    "                                output_col = output_col)\n",
    "\n",
    "        # run CV on this model\n",
    "        metrics = cv(df = df,\n",
    "                     train_test_fn = train_test_fn,\n",
    "                     folds = 5,\n",
    "                     random_state = 234234)\n",
    "        \n",
    "        # metrics is a list of length 5\n",
    "        all_metrics.append(metrics)\n",
    "\n",
    "    # organize all results in a 2D matrix \n",
    "    # (Rows correspond to combinations, and columns to indiviual CV runs)\n",
    "    all_metrics = np.array(all_metrics)\n",
    "\n",
    "    # compute the average CV performance of every combination\n",
    "    avg_metric = np.mean(all_metrics, axis=1) # size V (V is # of combinations)\n",
    "\n",
    "    # pick the best\n",
    "    best_ix = np.argmin(avg_metric) # the row index of the model with the least MSE\n",
    "\n",
    "    # this is the best combination\n",
    "    best_combination = all_combinations[best_ix]\n",
    "\n",
    "    return best_combination\n",
    "\n",
    "input_cols = ['Cylinders', \n",
    "              'Displacement', \n",
    "              'Horsepower', \n",
    "              'Weight_in_lbs', \n",
    "              'Acceleration',\n",
    "              'Origin',\n",
    "              'Year']\n",
    "\n",
    "brute_force_selection(df = df,\n",
    "                      input_cols = input_cols,\n",
    "                      output_col = 'Miles_per_Gallon',\n",
    "                      factory = factory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce250dd3-0d01-463d-b5ef-1d6d7f7afdad",
   "metadata": {},
   "source": [
    "# Forward Greedy Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e9245bc8-eb87-46d0-9c82-9cbddf7f3a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Weight_in_lbs', 'Year', 'Origin']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def greedy_selection(df, # dataset\n",
    "                     input_cols, # list of features to search over\n",
    "                     output_col, # output column\n",
    "                     factory): # factory\n",
    "\n",
    "    # we need to track the current set of features\n",
    "    # that we have selected so far\n",
    "    current_combination = []\n",
    "\n",
    "    # we also need to know what is the best metric value \n",
    "    # we have encountered so far\n",
    "    current_metric = np.inf\n",
    "\n",
    "    # and we need to know what are the remaining features that\n",
    "    # we need consider\n",
    "    remaining_features = input_cols # initially, its ALL the features\n",
    "\n",
    "    # As long as there are remaining features ... do the following\n",
    "    while len(remaining_features) > 0:\n",
    "\n",
    "        # track the performance of all candidate features\n",
    "        all_metrics = []\n",
    "\n",
    "        for feature in remaining_features:\n",
    "\n",
    "            # create the candidate feature set\n",
    "            # this is [what we have selected so far] + the feature\n",
    "            # For example, if current_combination = ['Horsepower']\n",
    "            # candidate = ['Horsepower'] + ['Cylinders'] = ['Horsepower', 'Cylinders']\n",
    "            candidate_combination = current_combination + [feature]\n",
    "\n",
    "            # create train test fn\n",
    "            train_test_fn = factory(input_cols = candidate_combination,\n",
    "                                    output_col = output_col)\n",
    "\n",
    "            # run CV on the candidate feature set\n",
    "            metrics = cv(df = df,\n",
    "                         train_test_fn = train_test_fn,\n",
    "                         folds = 5,\n",
    "                         random_state = 234234)\n",
    "            # metrics should be a list of length 5, containing the MSE\n",
    "            # of each CV fold\n",
    "            all_metrics.append(metrics)\n",
    "\n",
    "        # Now we are outside the inner loop\n",
    "\n",
    "        # Organize results into 2D Matrix \n",
    "        all_metrics = np.array(all_metrics) # V x 5 (V is # combinations)\n",
    "\n",
    "        # Compute average metric for each candidate feature\n",
    "        avg_metric = np.mean(all_metrics, axis=1) # V\n",
    "\n",
    "        # Pick the best feature in terms of metric\n",
    "        best_ix = np.argmin(avg_metric)\n",
    "        best_metric = avg_metric[best_ix] # the loss of the best possible next feature\n",
    "\n",
    "        # If the best metric we got at this point does not even improve\n",
    "        # upon the best we have so far ... STOP\n",
    "        # None of the features improved the performance of the model so far\n",
    "        if best_metric > current_metric:\n",
    "            break # stops the while loop\n",
    "\n",
    "        # if we are still here, that means that the best_metric <= current_metric\n",
    "        # so lets update our state\n",
    "\n",
    "        # new current best metric\n",
    "        current_metric = best_metric\n",
    "\n",
    "        # new best combination\n",
    "        best_next_feature = remaining_features[best_ix] \n",
    "        current_combination = current_combination + [best_next_feature]\n",
    "\n",
    "        # update the remaining features\n",
    "        remaining_features = [f for f in remaining_features if f != best_next_feature]\n",
    "        \n",
    "    # \"best\" combination\n",
    "    return current_combination\n",
    "\n",
    "input_cols = ['Cylinders', \n",
    "              'Displacement', \n",
    "              'Horsepower', \n",
    "              'Weight_in_lbs', \n",
    "              'Acceleration',\n",
    "              'Origin',\n",
    "              'Year']\n",
    "greedy_selection(df = df,\n",
    "                 input_cols = input_cols,\n",
    "                 output_col = 'Miles_per_Gallon',\n",
    "                 factory = factory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5e6fee-297f-47e7-b8b7-7b51365f7792",
   "metadata": {},
   "source": [
    "# Nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4fee2a35-c0da-467b-bdda-626c5bb9b571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Weight_in_lbs', 'Origin', 'Year']\n",
      "['Weight_in_lbs', 'Origin', 'Year']\n",
      "['Weight_in_lbs', 'Acceleration', 'Origin', 'Year']\n",
      "['Weight_in_lbs', 'Acceleration', 'Origin', 'Year']\n",
      "['Weight_in_lbs', 'Origin', 'Year']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[np.float64(10.702766968454496),\n",
       " np.float64(12.16392085857553),\n",
       " np.float64(7.634407766738132),\n",
       " np.float64(10.859485493381863),\n",
       " np.float64(10.85191612094271)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feature_selection_factory(input_cols, # features to consider\n",
    "                              output_col, # output\n",
    "                              method): # 'greedy' or 'brute'\n",
    "\n",
    "    # pick the strategy for doing feature selection\n",
    "    if method == 'greedy':\n",
    "        feature_selection_fn = greedy_selection\n",
    "    elif method == 'brute':\n",
    "        feature_selection_fn = brute_force_selection\n",
    "    else:\n",
    "        raise Exception(\"Unknown method. Please select 'greedy' or 'brute'\")\n",
    "\n",
    "    # now we define the function that we return to the caller\n",
    "    def train_test_fn(train_df, test_df):\n",
    "\n",
    "        # First, find the best feature combination\n",
    "        best_combination = feature_selection_fn(df = train_df,\n",
    "                                                input_cols = input_cols,\n",
    "                                                output_col = output_col,\n",
    "                                                factory = factory)\n",
    "        print(best_combination)\n",
    "        \n",
    "        # Then, fit the best combination to the whole training data frame\n",
    "        final_train_test_fn = factory(input_cols = best_combination,\n",
    "                                      output_col = output_col)\n",
    "        \n",
    "        # Evaluate the model on the test data frame\n",
    "        return final_train_test_fn(train_df, test_df)\n",
    "        \n",
    "    return train_test_fn \n",
    "\n",
    "input_cols = ['Cylinders', \n",
    "              'Displacement', \n",
    "              'Horsepower', \n",
    "              'Weight_in_lbs', \n",
    "              'Acceleration',\n",
    "              'Origin',\n",
    "              'Year']\n",
    "feature_selection_model_fn = feature_selection_factory(input_cols = input_cols,\n",
    "                                                       output_col = 'Miles_per_Gallon',\n",
    "                                                       method = 'brute')\n",
    "cv(df = df,\n",
    "   train_test_fn = feature_selection_model_fn,\n",
    "   folds = 5,\n",
    "   random_state = 32234) # this is the outer CV loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f69a51-ce34-4829-b5b1-608024fb50c2",
   "metadata": {},
   "source": [
    "# JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0ae6e86e-ad8a-47c5-acae-eee11763aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9f1b0097-dcea-4809-92dc-3b849290e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autodifferential capabilities of JAX\n",
    "\n",
    "jnp.sin(jnp.pi/3)\n",
    "\n",
    "# gradient of the sin function\n",
    "grad_fn = jax.grad(jnp.sin)\n",
    "grad_fn(jnp.pi/2) # cos(1)\n",
    "\n",
    "# gradient of a function that takes a parameter\n",
    "def myfunc(w):\n",
    "    return w * 2\n",
    "\n",
    "grad_fn = jax.grad(myfunc) # derivative of myfunc with respect to w\n",
    "grad_fn(10.)\n",
    "\n",
    "def myfunc(w, x):\n",
    "    return w*x\n",
    "\n",
    "grad_fn = jax.grad(myfunc) # derivative of myfunc with respect the first parameter\n",
    "                           # , which is w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9c196531-4d65-424a-9873-6c6a153a3ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b0': Array(1., dtype=float32, weak_type=True),\n",
       " 'b1': Array(15., dtype=float32, weak_type=True)}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our functions usually have more than one parameter\n",
    "def myfunc(params, x):\n",
    "    return (params['b0'] + params['b1'] * x)\n",
    "val = myfunc(params = { 'b0' : 1., \n",
    "                        'b1' : 5. }, \n",
    "             x = 10.)\n",
    "\n",
    "grad_fn = jax.grad(myfunc) # so this will create the gradient with respect to b0 and b1\n",
    "grad_fn({ 'b0' : 1., 'b1' : 5. }, x=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08cf870-77d3-4904-8fe2-acdcea33cedc",
   "metadata": {},
   "source": [
    "# Randomess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8594214e-d1a6-42a2-99a5-ca0a2e152a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02830462  0.46713185  0.29570296]\n",
      "[-0.02830462  0.46713185  0.29570296]\n",
      "[-0.02830462  0.46713185  0.29570296]\n",
      "[-0.02830462  0.46713185  0.29570296]\n",
      "[-0.02830462  0.46713185  0.29570296]\n",
      "[ 0.07592554 -0.48634264  1.2903206 ]\n",
      "[ 0.60576403  0.7990441  -0.908927  ]\n",
      "\n",
      "[ 0.07592554 -0.48634264  1.2903206 ]\n",
      "[ 0.60576403  0.7990441  -0.908927  ]\n",
      "[ 0.4323065  0.5872638 -1.1416743]\n",
      "[-0.2818947 -1.367489  -1.6350379]\n",
      "[0.6549178  0.17345214 1.6018405 ]\n",
      "[-0.2166012  -1.9878021  -0.61060226]\n",
      "[-0.25440374 -0.6385937  -0.68521845]\n",
      "[ 0.2886397  -0.00963292  0.15268941]\n",
      "[ 0.14384735 -0.15262456 -1.7989424 ]\n",
      "[-1.3462586   0.5520057  -0.75974613]\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.key(42) # initializing a random \"key\" with value 42\n",
    "\n",
    "# let's generate some standard normal variates\n",
    "print(jax.random.normal(key, 3)) # generates three values\n",
    "print(jax.random.normal(key, 3)) # generates three values\n",
    "print(jax.random.normal(key, 3)) # generates three values\n",
    "print(jax.random.normal(key, 3)) # generates three values\n",
    "print(jax.random.normal(key, 3)) # generates three values\n",
    "\n",
    "key1, key2 = jax.random.split(key, 2) # split key into two new keys\n",
    "print(jax.random.normal(key1, 3)) # generates three values\n",
    "print(jax.random.normal(key2, 3)) # generates three values\n",
    "print()\n",
    "for i in range(10):\n",
    "    \n",
    "    loop_key = jax.random.fold_in(key, i)\n",
    "    #loop_key, _ = jax.random.split(loop_key, 2)\n",
    "    \n",
    "    val = jax.random.normal(loop_key, 3)\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0fb4de-ec22-41b0-9a68-bb062a3a966b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
