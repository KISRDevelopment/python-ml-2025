{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79fd0b21",
   "metadata": {},
   "source": [
    "# Fighting Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "543594f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import numpy.random as rng\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "d06626d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_inputs(input_df):\n",
    "    \"\"\"\n",
    "        Prepares the input features that will be fed into the model.\n",
    "\n",
    "        Inputs:\n",
    "            input_df: the input dataframe into the function. Should consist ONLY of input features.\n",
    "        Outputs:\n",
    "            Z: the input feature matrix of size NxK, where K is the number of features\n",
    "    \"\"\"\n",
    "    # Let's identify categorical columns in a dataframe\n",
    "    categorical_cols = input_df.select_dtypes(include='object').columns\n",
    "    \n",
    "    # Let's identify the numeric columns in the dataframe\n",
    "    numeric_cols = input_df.select_dtypes(include='number').columns\n",
    "\n",
    "    # We want to construct the input features into the model\n",
    "    # We will use a numpy array that contains both numeric and categorically encoded values\n",
    "    X = input_df[numeric_cols].to_numpy() # (NxK)\n",
    "    \n",
    "    # Now we need to z-score the numeric features so that they can lead to efficient learning\n",
    "    col_means = np.mean(X, axis=0) # K\n",
    "    col_stds = np.std(X, axis=0, ddof=1) # K\n",
    "    \n",
    "    # Z-score\n",
    "    # (NxK - \n",
    "    #  1xK) \n",
    "    #  / \n",
    "    #  (1xK)\n",
    "    Z = (X - col_means[None, :]) / col_stds[None, :]\n",
    "    \n",
    "    # Now we want to code the categorical columns using one-hot encoding\n",
    "    for col in categorical_cols:\n",
    "        # NxC (C is the number of unique values in the column)\n",
    "        # So for origin this will be Nx3 \n",
    "        dummies = pd.get_dummies( input_df[col] ).to_numpy() \n",
    "        \n",
    "        # concatenate dummies matrix onto Z\n",
    "        #print(Z.shape)\n",
    "        #print(dummies.shape)\n",
    "        Z = np.hstack((Z, dummies)) \n",
    "    \n",
    "    # finally we want to add a column of ones at the start of Z\n",
    "    ones_col = np.ones((Z.shape[0], 1)) # Nx1\n",
    "    \n",
    "    Z = np.hstack((ones_col, Z))\n",
    "\n",
    "    return Z\n",
    "\n",
    "def forward_fn(params, Z):\n",
    "    \"\"\"\n",
    "        The MLP forward function.\n",
    "        This is a neuron network with one hidden layer and ReLU activations.\n",
    "        \n",
    "        Inputs:\n",
    "            params: the weight of the model\n",
    "            Z: the input feature matrix, as returned by prepare_inputs (size NxK)\n",
    "        Output:\n",
    "            yhat: the model's predictions (size N)\n",
    "    \"\"\"\n",
    "    # compute the inputs into the neurons\n",
    "    # 1xH + Nxd @ dxH = 1xH + NxH = NxH\n",
    "    hidden_inputs = params['b_hidden'][None,:] + Z @ params['W_input_hidden']\n",
    "    \n",
    "    # next, we apply the non-linearity to those inputs\n",
    "    #hidden_activations = jnp.tanh(hidden_inputs) # NxH\n",
    "    hidden_activations = jnp.maximum(0, hidden_inputs) # <-- ReLU\n",
    "    \n",
    "    # now we compute the output\n",
    "    # 1 + NxH @ H = 1 + N = N \n",
    "    f = params['b_output'] + hidden_activations @ params['W_hidden_output']\n",
    "    p = 1/(1+jnp.exp(-f)) # <-- sigmoid function (the probability of a positive)\n",
    "\n",
    "    # so that the log doesn't blow up\n",
    "    p = jnp.clip(p, 0.01, 0.99)\n",
    "\n",
    "    return p\n",
    "\n",
    "def predict(params, input_df):\n",
    "    \"\"\"\n",
    "        Convienience function that prepares inputs and runs the forward function.\n",
    "\n",
    "        Inputs:\n",
    "            params: the weights of the model\n",
    "            input_df: input data frame (input features only, no output column).\n",
    "        Output:\n",
    "            yhat: the model's predictions (size N)\n",
    "    \"\"\"\n",
    "    Z = prepare_inputs(input_df)\n",
    "    return forward_fn(params, Z)\n",
    "\n",
    "def loss_fn(params, Z, y, penalty):\n",
    "    \"\"\"\n",
    "        Computes the mean squared error loss function for the model.\n",
    "\n",
    "        Inputs:\n",
    "            params: the weights of the model\n",
    "            Z: the input feature matrix, as returned by prepare_inputs (size NxK)\n",
    "            y: actual observations (size N)\n",
    "            penalty: penalty term for l2 regularization\n",
    "        Output:\n",
    "            mse: mean squared error\n",
    "    \"\"\"\n",
    "    p = forward_fn(params, Z) # N\n",
    "\n",
    "    # Log Probability of each data point, given the model (shape N)\n",
    "    log_probability_of_data = y * jnp.log(p) + (1-y) * jnp.log(1-p)\n",
    "\n",
    "    # We want to maximize the log probability of the data under the model\n",
    "    # but gradient descent minimizes a loss\n",
    "    # So we want to minimize the negative log probability\n",
    "\n",
    "    sum_weights_squared = jnp.sum(jnp.square(params['W_input_hidden'])) + jnp.sum(jnp.square(params['W_hidden_output']))\n",
    "    loss = -jnp.mean(log_probability_of_data) + penalty * sum_weights_squared\n",
    "    \n",
    "    return loss \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820aa923",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "3bcd57ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize(rng, input_df, y, learning_rate, epochs, n_hidden, penalty):\n",
    "    \"\"\"\n",
    "        Input parameters:\n",
    "            rng: the random key\n",
    "            input_df: dataframe containing input columns\n",
    "            y: a vector of outputs that we wish to predict\n",
    "            learning_rate: how quickly we want gradient descent learning\n",
    "            epochs: the number of steps of gradient descent\n",
    "            n_hidden: the number of hidden units\n",
    "            penalty: penalty for l2 regularization\n",
    "        Output:\n",
    "            params: fitted model parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # To make this work, we need to convert y to jax\n",
    "    y = jnp.array(y)\n",
    "    \n",
    "    # the magic: the gradient function\n",
    "    # Creates a function that can calculate the  gradient of the model\n",
    "    grad_fn = jax.grad(loss_fn) \n",
    "    \n",
    "    # Prepare our inputs into the linear regression\n",
    "    Z = prepare_inputs(input_df) # NxK\n",
    "\n",
    "    #\n",
    "    # Initialize the parameters\n",
    "    #\n",
    "    params = {} # initialize an empty dictionary\n",
    "    # weights from inputs to hidden neurons (dxH)\n",
    "    n_inputs = Z.shape[1]\n",
    "    params['W_input_hidden'] = jax.random.normal(rng, (n_inputs, n_hidden)) / jnp.sqrt(n_inputs)\n",
    "    rng, _ = jax.random.split(rng)  # move to the next random key\n",
    "    \n",
    "    # bias of the hidden neurons is initialized to zero\n",
    "    params['b_hidden'] = jnp.zeros(n_hidden)\n",
    "    \n",
    "    # weights from the hidden neurons to the output neuron (shape is H)\n",
    "    params['W_hidden_output'] = jax.random.normal(rng, (n_hidden,)) / jnp.sqrt(n_hidden)\n",
    "    rng, _ = jax.random.split(rng) # move to next random key\n",
    "    \n",
    "    # finally, initialize the bias of the output neuron \n",
    "    params['b_output'] = 0.0\n",
    "    \n",
    "    # Run gradient descent loop\n",
    "    for i in range(epochs):\n",
    "\n",
    "        # Compute the gradient of the loss function with respect\n",
    "        # to all model parameters\n",
    "        W_grad = grad_fn(params, Z, y, penalty)\n",
    "        \n",
    "        # Update the parameters\n",
    "        for key in params:\n",
    "            params[key] = params[key] - W_grad[key] * learning_rate\n",
    "\n",
    "    # params is the fitted parameter values\n",
    "    return params\n",
    "\n",
    "def mlp_train_test_function(rng,\n",
    "                            train_df, \n",
    "                            test_df, \n",
    "                            input_cols, \n",
    "                            output_col,\n",
    "                            n_hidden,\n",
    "                            penalty):\n",
    "    \"\"\"\n",
    "        Function that trains an MLP and tests it. Returns multiple evaluation metrics.\n",
    "\n",
    "        Inputs:\n",
    "            rng: Random number key\n",
    "            train_df: training data frame\n",
    "            test_df: testing data frame \n",
    "            input_cols: features to use\n",
    "            output_col: output to predict\n",
    "            n_hidden: number of hidden units \n",
    "            penalty: L2 penalty term\n",
    "        Outputs:\n",
    "            results: A dictionary containing accuracy, accuracy_null, auc_roc, auc_pr, auc_pr_null\n",
    "    \"\"\"\n",
    "    # build the training input data frame\n",
    "    train_input_df = train_df[input_cols]\n",
    "\n",
    "    # build the training outputs\n",
    "    y = train_df[output_col].to_numpy()\n",
    "    \n",
    "    # Optimize the model using gradient descent\n",
    "    best_params = optimize(rng = rng,\n",
    "                           input_df = train_input_df,\n",
    "                           y = y,\n",
    "                           learning_rate = 0.1,\n",
    "                           epochs = 500,\n",
    "                           n_hidden = n_hidden,\n",
    "                           penalty=penalty)\n",
    "\n",
    "    # build the testing input data frame\n",
    "    test_input_df = test_df[input_cols]\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    yhat = predict(params = best_params, input_df = test_input_df)\n",
    "    \n",
    "    # Calculate error of those predictions\n",
    "    ytest = test_df[output_col].to_numpy()\n",
    "\n",
    "    # We will return multiple metrics\n",
    "    yhat_hard = yhat > 0.5\n",
    "\n",
    "    # this is our null model, and it is based only on the training set\n",
    "    yhat_null = jnp.mean(y) * jnp.ones_like(ytest)\n",
    "    yhat_hard_null = yhat_null > 0.5\n",
    "    \n",
    "    return dict(\n",
    "        accuracy = sklearn.metrics.accuracy_score(ytest, yhat_hard),\n",
    "        accuracy_null = sklearn.metrics.accuracy_score(ytest, yhat_hard_null),\n",
    "        auc_roc = sklearn.metrics.roc_auc_score(ytest, yhat), # soft decision\n",
    "        auc_pr = sklearn.metrics.average_precision_score(ytest, yhat),\n",
    "        auc_pr_null = sklearn.metrics.average_precision_score(ytest, yhat_null)\n",
    "    )\n",
    "\n",
    "def factory(rng, input_cols, output_col, n_hidden, penalty):\n",
    "\n",
    "    def train_test_fn(train_df, test_df):\n",
    "\n",
    "        return mlp_train_test_function(rng,\n",
    "                                  train_df = train_df,\n",
    "                                  test_df = test_df,\n",
    "                                  input_cols = input_cols,\n",
    "                                  output_col = output_col,\n",
    "                                  n_hidden=n_hidden,\n",
    "                                  penalty=penalty)\n",
    "\n",
    "    return train_test_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "eaec107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(df, train_test_fn, folds, random_state):\n",
    "    \"\"\"\n",
    "        Cross-validation: splits dataset into N folds, repeatedly trains on N-1 folds and test on the remaining.\n",
    "\n",
    "        Inputs:\n",
    "            df: dataframe of inputs and outputs\n",
    "            train_test_fn: the training and testing function used\n",
    "            folds: number of folds\n",
    "            random_state: pseudo random number generator state\n",
    "        Output:\n",
    "            metrics: loss on each split (size N)\n",
    "    \n",
    "    \"\"\"\n",
    "    # instantiate the splitter\n",
    "    kf = sklearn.model_selection.KFold(n_splits=folds, \n",
    "                                       shuffle=True, \n",
    "                                       random_state=random_state)\n",
    "    \n",
    "    metrics = []\n",
    "    for train_index, test_index in kf.split(df):\n",
    "        train_df = df.iloc[train_index]\n",
    "        test_df = df.iloc[test_index]\n",
    "        \n",
    "        # evaluate\n",
    "        metric = train_test_fn(train_df, test_df)\n",
    "        metrics.append(metric)\n",
    "    \n",
    "    return metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "b6a81b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy         0.849000\n",
       "accuracy_null    0.481000\n",
       "auc_roc          0.930357\n",
       "auc_pr           0.928479\n",
       "auc_pr_null      0.498000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"../data/overfitting.csv\")\n",
    "\n",
    "# get all columns, except last one\n",
    "input_cols = df.columns[:-1]\n",
    "\n",
    "# get last column\n",
    "output_col = df.columns[-1]\n",
    "\n",
    "rng = jax.random.key(42)\n",
    "\n",
    "train_test_fn = factory(rng, input_cols, output_col, n_hidden=20, penalty=0.01)\n",
    "\n",
    "results = cv(df = df,\n",
    "             train_test_fn = train_test_fn,\n",
    "             folds = 5,\n",
    "             random_state = 234234)\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "adf9441c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy         0.831500\n",
       "accuracy_null    0.481000\n",
       "auc_roc          0.916220\n",
       "auc_pr           0.912924\n",
       "auc_pr_null      0.498000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_fn = factory(rng, input_cols, output_col, n_hidden=20, penalty=0.00)\n",
    "\n",
    "results = cv(df = df,\n",
    "             train_test_fn = train_test_fn,\n",
    "             folds = 5,\n",
    "             random_state = 234234)\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4044a101",
   "metadata": {},
   "source": [
    "# Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "df9856c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping at epoch 226\n",
      "Stopping at epoch 242\n",
      "Stopping at epoch 198\n",
      "Stopping at epoch 230\n",
      "Stopping at epoch 210\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy         0.839500\n",
       "accuracy_null    0.481000\n",
       "auc_roc          0.921996\n",
       "auc_pr           0.914948\n",
       "auc_pr_null      0.498000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def optimize(rng, input_df, y, learning_rate, epochs, n_hidden, early_stopping_patience=50, validation_prop=0.2):\n",
    "    \"\"\"\n",
    "        Input parameters:\n",
    "            rng: the random key\n",
    "            input_df: dataframe containing input columns\n",
    "            y: a vector of outputs that we wish to predict\n",
    "            learning_rate: how quickly we want gradient descent learning\n",
    "            epochs: the number of steps of gradient descent\n",
    "            n_hidden: the number of hidden units\n",
    "            early_stopping_patience (optional): number of epochs without a new best to wait before stopping. \n",
    "            validation_prop: proportion of dataset to reserve for validation for early stopping.\n",
    "        Output:\n",
    "            params: fitted model parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # To make this work, we need to convert y to jax\n",
    "    y = jnp.array(y)\n",
    "    \n",
    "    # the magic: the gradient function\n",
    "    # Creates a function that can calculate the  gradient of the model\n",
    "    grad_fn = jax.grad(loss_fn) \n",
    "    \n",
    "    # Prepare our inputs into the linear regression\n",
    "    Z = prepare_inputs(input_df) # NxK\n",
    "\n",
    "    #\n",
    "    # Initialize the parameters\n",
    "    #\n",
    "    params = {} # initialize an empty dictionary\n",
    "    # weights from inputs to hidden neurons (dxH)\n",
    "    n_inputs = Z.shape[1]\n",
    "    params['W_input_hidden'] = jax.random.normal(rng, (n_inputs, n_hidden)) / jnp.sqrt(n_inputs)\n",
    "    rng, _ = jax.random.split(rng)  # move to the next random key\n",
    "    \n",
    "    # bias of the hidden neurons is initialized to zero\n",
    "    params['b_hidden'] = jnp.zeros(n_hidden)\n",
    "    \n",
    "    # weights from the hidden neurons to the output neuron (shape is H)\n",
    "    params['W_hidden_output'] = jax.random.normal(rng, (n_hidden,)) / jnp.sqrt(n_hidden)\n",
    "    rng, _ = jax.random.split(rng) # move to next random key\n",
    "    \n",
    "    # finally, initialize the bias of the output neuron \n",
    "    params['b_output'] = 0.0\n",
    "    \n",
    "    #\n",
    "    # Early Stopping Preparation:\n",
    "    #   Split the training data into an optimization and validation portions.\n",
    "    #   \n",
    "    optimization_prop = 1-validation_prop\n",
    "    n_opt = int(optimization_prop * y.shape[0])\n",
    "\n",
    "    idx = jax.random.permutation(rng, y.shape[0])\n",
    "    opt_idx = idx[:n_opt]\n",
    "    valid_idx = idx[n_opt:]\n",
    "\n",
    "    y_opt, y_valid = y[opt_idx], y[valid_idx]\n",
    "    Z_opt, Z_valid = Z[opt_idx,:], Z[valid_idx]\n",
    "\n",
    "    epochs_since_last_best = 0\n",
    "    best_valid_loss = jnp.inf \n",
    "    best_params = None \n",
    "\n",
    "    # Run gradient descent loop\n",
    "    for i in range(epochs):\n",
    "\n",
    "        # Compute the gradient of the loss function with respect\n",
    "        # to all model parameters\n",
    "        W_grad = grad_fn(params, Z_opt, y_opt, 0.)\n",
    "        \n",
    "        # Update the parameters\n",
    "        for key in params:\n",
    "            params[key] = params[key] - W_grad[key] * learning_rate\n",
    "\n",
    "        # Compute validation loss\n",
    "        valid_loss = loss_fn(params, Z_valid, y_valid, 0.)\n",
    "        \n",
    "        # is it new best?\n",
    "        if valid_loss < best_valid_loss:\n",
    "            #print(f\"New best @ {i}: {valid_loss}\")\n",
    "            # it is, so record params and reset counter\n",
    "            best_valid_loss = valid_loss\n",
    "            best_params = copy.deepcopy(params) # <-- must do this, otherwise best params will keep changing in subsequent iterations\n",
    "            epochs_since_last_best = 0\n",
    "        else:\n",
    "            epochs_since_last_best += 1\n",
    "\n",
    "            # it is not, so check if we have ran out of patience\n",
    "            if epochs_since_last_best == early_stopping_patience:\n",
    "                print(f\"Stopping at epoch {i}\")\n",
    "                break\n",
    "        #best_params = params \n",
    "\n",
    "    # params is the fitted parameter values\n",
    "    return best_params\n",
    "\n",
    "def mlp_train_test_function(rng,\n",
    "                            train_df, \n",
    "                            test_df, \n",
    "                            input_cols, \n",
    "                            output_col,\n",
    "                            n_hidden,\n",
    "                            penalty):\n",
    "    \"\"\"\n",
    "        Function that trains an MLP and tests it. Returns multiple evaluation metrics.\n",
    "\n",
    "        Inputs:\n",
    "            rng: Random number key\n",
    "            train_df: training data frame\n",
    "            test_df: testing data frame \n",
    "            input_cols: features to use\n",
    "            output_col: output to predict\n",
    "            n_hidden: number of hidden units \n",
    "            penalty: unused here\n",
    "        Outputs:\n",
    "            results: A dictionary containing accuracy, accuracy_null, auc_roc, auc_pr, auc_pr_null\n",
    "    \"\"\"\n",
    "    # build the training input data frame\n",
    "    train_input_df = train_df[input_cols]\n",
    "\n",
    "    # build the training outputs\n",
    "    y = train_df[output_col].to_numpy()\n",
    "    \n",
    "    # Optimize the model using gradient descent\n",
    "    best_params = optimize(rng = rng,\n",
    "                           input_df = train_input_df,\n",
    "                           y = y,\n",
    "                           learning_rate = 0.5,\n",
    "                           epochs = 500,\n",
    "                           n_hidden = n_hidden)\n",
    "\n",
    "    # build the testing input data frame\n",
    "    test_input_df = test_df[input_cols]\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    yhat = predict(params = best_params, input_df = test_input_df)\n",
    "    \n",
    "    # Calculate error of those predictions\n",
    "    ytest = test_df[output_col].to_numpy()\n",
    "\n",
    "    # We will return multiple metrics\n",
    "    yhat_hard = yhat > 0.5\n",
    "\n",
    "    # this is our null model, and it is based only on the training set\n",
    "    yhat_null = jnp.mean(y) * jnp.ones_like(ytest)\n",
    "    yhat_hard_null = yhat_null > 0.5\n",
    "    \n",
    "    return dict(\n",
    "        accuracy = sklearn.metrics.accuracy_score(ytest, yhat_hard),\n",
    "        accuracy_null = sklearn.metrics.accuracy_score(ytest, yhat_hard_null),\n",
    "        auc_roc = sklearn.metrics.roc_auc_score(ytest, yhat), # soft decision\n",
    "        auc_pr = sklearn.metrics.average_precision_score(ytest, yhat),\n",
    "        auc_pr_null = sklearn.metrics.average_precision_score(ytest, yhat_null)\n",
    "    )\n",
    "\n",
    "train_test_fn = factory(rng, input_cols, output_col, n_hidden=20, penalty=0.00)\n",
    "\n",
    "results = cv(df = df,\n",
    "             train_test_fn = train_test_fn,\n",
    "             folds = 5,\n",
    "             random_state = 234234)\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40415f15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-ml-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
