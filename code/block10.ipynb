{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaf39533",
   "metadata": {},
   "source": [
    "# Binary Classifier Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bba2705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from typing import List\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bf3a3f",
   "metadata": {},
   "source": [
    "## Trying Out the Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcaa6e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some dummy observations and predictions\n",
    "y_actual = np.array([0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0]) # N\n",
    "y_pred = np.array([0.06, 0.92, 0.86, 0.03, 0.40, 0.70, 0.23, 0.4, 0.2, 0.8, 0.9, 0.65, 0.75, 0.4]) # N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "073e840c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  0.7142857142857143\n",
      "Accuracy (Null)  0.6428571428571429\n",
      "Balanced Accuracy  0.7333333333333334\n",
      "Balanced Accuracy (Null)  0.5\n",
      "Recall  0.6666666666666666\n",
      "Recall (Ones all the time)  1.0\n",
      "Precision  0.8571428571428571\n",
      "Precision (Ones all the time)  0.6428571428571429\n",
      "Precision (Trivial)  1.0\n",
      "F1  0.75\n",
      "F1 (Ones all the time) 0.782608695652174\n",
      "AUC-ROC  0.7555555555555555\n",
      "AUC-PR  0.8063492063492064\n",
      "AUC-PR (Null)  0.6428571428571429\n"
     ]
    }
   ],
   "source": [
    "# accuracy, notice that we have to threshold\n",
    "print(\"Accuracy \", sklearn.metrics.accuracy_score(y_actual, y_pred > 0.5))\n",
    "\n",
    "# accuracy under null model\n",
    "# in real world, the mean would be based on TRAINING set\n",
    "null_ypred = np.mean(y_actual) * np.ones_like(y_actual)\n",
    "print(\"Accuracy (Null) \", sklearn.metrics.accuracy_score(y_actual, null_ypred > 0.5))\n",
    "\n",
    "# balanced accuracy\n",
    "print(\"Balanced Accuracy \", sklearn.metrics.balanced_accuracy_score(y_actual, y_pred > 0.5))\n",
    "\n",
    "print(\"Balanced Accuracy (Null) \", sklearn.metrics.balanced_accuracy_score(y_actual, null_ypred > 0.5))\n",
    "\n",
    "\n",
    "# recall\n",
    "print(\"Recall \", sklearn.metrics.recall_score(y_actual, y_pred > 0.5))\n",
    "\n",
    "# recall when saying yes all the time\n",
    "print(\"Recall (Ones all the time) \", sklearn.metrics.recall_score(y_actual, np.ones_like(y_actual)))\n",
    "\n",
    "# precision\n",
    "print(\"Precision \", sklearn.metrics.precision_score(y_actual, y_pred > 0.5))\n",
    "\n",
    "# precision when saying yes all the time\n",
    "print(\"Precision (Ones all the time) \", sklearn.metrics.precision_score(y_actual, np.ones_like(y_actual)))\n",
    "\n",
    "\n",
    "# precision when predicting one positive example that happens to be correct\n",
    "yhat = np.zeros_like(y_actual)\n",
    "yhat[1] = 1\n",
    "print(\"Precision (Trivial) \", sklearn.metrics.precision_score(y_actual, yhat))\n",
    "\n",
    "# f1 score\n",
    "print(\"F1 \", sklearn.metrics.f1_score(y_actual, y_pred > 0.5))\n",
    "print(\"F1 (Ones all the time)\", sklearn.metrics.f1_score(y_actual, np.ones_like(y_actual)))\n",
    "\n",
    "# AUC-ROC, notice: no thresholding\n",
    "print(\"AUC-ROC \", sklearn.metrics.roc_auc_score(y_actual, y_pred))\n",
    "print(\"AUC-PR \", sklearn.metrics.average_precision_score(y_actual, y_pred))\n",
    "print(\"AUC-PR (Null) \", sklearn.metrics.average_precision_score(y_actual, np.mean(y_actual) * np.ones_like(y_actual)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc326d2",
   "metadata": {},
   "source": [
    "## Applying to Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eef54ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DF shape  (800, 3)\n"
     ]
    }
   ],
   "source": [
    "def forward_fn(Beta, X):\n",
    "    f = X @ Beta \n",
    "    p = 1/(1+jnp.exp(-f))\n",
    "    return p \n",
    "\n",
    "def loss_fn(Beta, X, y):\n",
    "    p = forward_fn(Beta, X)\n",
    "    loss = -jnp.mean(y * jnp.log(p) + (1-y) * jnp.log(1-p))\n",
    "    return loss \n",
    "\n",
    "class BinaryLinearModel():\n",
    "\n",
    "    def __init__(self, \n",
    "                 features: List[str]):\n",
    "        self._features = features \n",
    "    \n",
    "    def train(self, rng, \n",
    "              df: pd.DataFrame, \n",
    "              y: np.ndarray, \n",
    "              epochs: int = 100, \n",
    "              eta: float = 0.01, \n",
    "              batch_size: int = 100000):\n",
    "        grad_fn = jax.grad(loss_fn)\n",
    "\n",
    "        y = jnp.array(y)\n",
    "        \n",
    "        # prepare inputs and outputs\n",
    "        X = self._prepare_input_matrix(df[self._features])\n",
    "        \n",
    "        # randomly initialize solution \n",
    "        Beta = jax.random.normal(rng, X.shape[1]) # K\n",
    "\n",
    "        # iterate for epochs\n",
    "        history = []\n",
    "        for i in range(epochs):\n",
    "\n",
    "            # shuffle dataset (important)\n",
    "            loop_key = jax.random.fold_in(rng, i)\n",
    "            ix = jax.random.permutation(loop_key, X.shape[0])\n",
    "            X = X[ix, :]\n",
    "            y = y[ix]\n",
    "\n",
    "            # go over mini batches and update\n",
    "            for j in range(0, X.shape[0], batch_size):\n",
    "                offset = j \n",
    "                end = j + batch_size\n",
    "\n",
    "                # compute gradient\n",
    "                # this is very powerful ... JAX takes care of derivative computation\n",
    "                # so loss_fn could be as complex as you like\n",
    "                Beta_grad = grad_fn(Beta, X[offset:end,:], y[offset:end])\n",
    "                \n",
    "                # update solution\n",
    "                Beta = Beta - eta * Beta_grad\n",
    "\n",
    "            # record epoch loss\n",
    "            mse = loss_fn(Beta, X, y)\n",
    "            history.append([Beta, mse])\n",
    "\n",
    "        # save the parameters\n",
    "        self._params, _ = history[-1]\n",
    "\n",
    "        return history\n",
    "    \n",
    "    def _prepare_input_matrix(self, df: pd.DataFrame):\n",
    "\n",
    "        # we need to separate categorical from numeric features\n",
    "        # because they require separate processing\n",
    "        # let's get categorical columns\n",
    "        categorical_cols = df.select_dtypes(include='object').columns\n",
    "        \n",
    "        # let's get numeric\n",
    "        ordinal_cols = df.select_dtypes(include='number').columns\n",
    "\n",
    "        # construct input features\n",
    "        X = df[ordinal_cols].to_numpy()\n",
    "\n",
    "        # z-score (NxK' - 1xK') / 1xK' = NxK'\n",
    "        X = (X - np.mean(X, axis=0)[None, :]) / np.std(X, axis=0)[None, :]\n",
    "\n",
    "        # code categorical features\n",
    "        for feature in categorical_cols:\n",
    "            dummies = pd.get_dummies(df[feature]).to_numpy().astype(float)\n",
    "            X = np.hstack((X, dummies)) \n",
    "\n",
    "        # add a column of ones\n",
    "        ones_col = np.ones((X.shape[0], 1)) # Nx1\n",
    "        X = np.hstack((ones_col, X)) # K\n",
    "        \n",
    "        return jnp.array(X) \n",
    "    \n",
    "    def predict(self, df: pd.DataFrame):\n",
    "         \n",
    "        X = self._prepare_input_matrix(df[self._features])\n",
    "\n",
    "        return forward_fn(self._params, X)\n",
    "    \n",
    "df = pd.read_csv(\"../data/separable_binary_data.csv\")\n",
    "df\n",
    "\n",
    "\n",
    "rng = jax.random.key(52345)\n",
    "\n",
    "#\n",
    "# let's randomly split the data\n",
    "#\n",
    "\n",
    "# first, generate a shuffled permutation of indecies\n",
    "ix = jax.random.permutation(rng, df.shape[0])\n",
    "rng, _ = jax.random.split(rng)\n",
    "\n",
    "# grab 80% of the shuffled data for training, rest is for testing\n",
    "n_train = int(0.8 * df.shape[0])\n",
    "train_ix = ix[:n_train]\n",
    "test_ix = ix[n_train:]\n",
    "\n",
    "train_df = df.iloc[train_ix]\n",
    "test_df = df.iloc[test_ix]\n",
    "\n",
    "model = BinaryLinearModel(['x1', 'x2'])\n",
    "\n",
    "print(\"Train DF shape \", train_df.shape)\n",
    "history = model.train(rng, train_df, train_df['y'].to_numpy(), epochs=20, eta=0.1, batch_size=10000)\n",
    "\n",
    "# calculate null model on training data\n",
    "mu = np.mean(train_df['y'].to_numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8085dbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test DF shape  (200, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7,\n",
       " 'accuracy_null': 0.485,\n",
       " 'balanced_accuracy': 0.7012311079971976,\n",
       " 'balanced_accuracy_null': 0.5,\n",
       " 'recall': 0.7422680412371134,\n",
       " 'recall_null': 1.0,\n",
       " 'precision': 0.6728971962616822,\n",
       " 'precision_null': 0.485,\n",
       " 'f1': 0.7058823529411765,\n",
       " 'f1_null': 0.6531986531986532,\n",
       " 'auc_roc': 0.7746972275047542,\n",
       " 'auc_pr': 0.7838629030870375,\n",
       " 'auc_pr_null': 0.485}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict on test\n",
    "print(\"Test DF shape \", test_df.shape)\n",
    "yhat = model.predict(test_df)\n",
    "\n",
    "# null model prediction\n",
    "yhat_null = mu * np.ones(test_df.shape[0])\n",
    "\n",
    "# hard decisions ...\n",
    "threshold = 0.5 \n",
    "yhat_hard = yhat > threshold\n",
    "yhat_null_hard = yhat_null > threshold\n",
    "\n",
    "ytrue = test_df['y'].to_numpy()\n",
    "\n",
    "dict(\n",
    "    accuracy = sklearn.metrics.accuracy_score(ytrue, yhat_hard),\n",
    "    accuracy_null = sklearn.metrics.accuracy_score(ytrue, yhat_null_hard),\n",
    "    \n",
    "    balanced_accuracy = sklearn.metrics.balanced_accuracy_score(ytrue, yhat_hard),\n",
    "    balanced_accuracy_null = sklearn.metrics.balanced_accuracy_score(ytrue, yhat_null_hard),\n",
    "\n",
    "    recall = sklearn.metrics.recall_score(ytrue, yhat_hard),\n",
    "    recall_null = sklearn.metrics.recall_score(ytrue, yhat_null_hard),\n",
    "\n",
    "    precision = sklearn.metrics.precision_score(ytrue, yhat_hard),\n",
    "    precision_null = sklearn.metrics.precision_score(ytrue, yhat_null_hard),\n",
    "\n",
    "    f1 = sklearn.metrics.f1_score(ytrue, yhat_hard),\n",
    "    f1_null = sklearn.metrics.f1_score(ytrue, yhat_null_hard),\n",
    "\n",
    "    auc_roc = sklearn.metrics.roc_auc_score(ytrue, yhat),\n",
    "    auc_pr = sklearn.metrics.average_precision_score(ytrue, yhat),\n",
    "    auc_pr_null = sklearn.metrics.average_precision_score(ytrue, yhat_null),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4055914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-ml-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
