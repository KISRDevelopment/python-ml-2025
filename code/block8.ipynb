{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a82a62b",
   "metadata": {},
   "source": [
    "# Block 8: JAX\n",
    "\n",
    "Wouldn't it be nice if we didn't have to do the derivative calculations manually? JAX is very similar to numpy, but it has a number of extra important capabilities, the most important of which is auto differentiation of arbitrary computations.\n",
    "\n",
    "I had a difficult choice at this juncture: whether to introduce you to the more common `pytorch` library, or whether to use the less known `jax`. The first is widely used and somewhat similar to numpy, but introduces a number of new concepts. `jax` is actually faster, and I feel that introduces minimal changes which will hopefully be easier to appreciate within the short time span of this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c385c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from typing import List\n",
    "import jax.numpy as jnp \n",
    "import jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dc4eec",
   "metadata": {},
   "source": [
    "## Autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d1ef9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b0': Array(1., dtype=float32, weak_type=True),\n",
       " 'b1': Array(10., dtype=float32, weak_type=True)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple gradient\n",
    "grad_fn = jax.grad(jax.numpy.sin)\n",
    "grad_fn(0.)\n",
    "grad_fn = jax.grad(jax.numpy.cos)\n",
    "grad_fn(0.)\n",
    "\n",
    "# gradient of a function that takes a parameter\n",
    "def some_fn(w):\n",
    "    return w * 2\n",
    "\n",
    "grad_fn = jax.grad(some_fn)\n",
    "grad_fn(4.)\n",
    "\n",
    "# gradient of a function that takes a parameter + an input\n",
    "def some_fn(w, x):\n",
    "    return w*x\n",
    "\n",
    "grad_fn = jax.grad(some_fn)\n",
    "grad_fn(4., 10)\n",
    "\n",
    "# gradient of a function that takes multiple parameters\n",
    "def some_fn(p, x):\n",
    "    return p['b0'] + p['b1'] * x \n",
    "\n",
    "grad_fn = jax.grad(some_fn)\n",
    "grad_fn({ \"b0\" : 1., \"b1\" : 3.5 }, 10.) # neat! will return derivative with respect to b0 and with respect to b1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9d2213",
   "metadata": {},
   "source": [
    "## Randomness\n",
    "\n",
    "JAX is explicit about random number generation. There is no magical \"global\" random number generator seed. Instead, any function that involves any kind of randomness, but receive a \"key\" that controls how the random numbers are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efb58948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02830462  0.46713185  0.29570296]\n",
      "[-0.02830462  0.46713185  0.29570296]\n",
      "[ 0.07592554 -0.48634264  1.2903206 ]\n",
      "[ 0.07592554 -0.48634264  1.2903206 ]\n",
      "[ 0.07592554 -0.48634264  1.2903206 ]\n",
      "[ 0.07592554 -0.48634264  1.2903206 ]\n",
      "[ 0.07592554 -0.48634264  1.2903206 ]\n",
      "[ 0.07592554 -0.48634264  1.2903206 ]\n",
      "[ 0.07592554 -0.48634264  1.2903206 ]\n",
      "[ 0.07592554 -0.48634264  1.2903206 ]\n",
      "[ 0.07592554 -0.48634264  1.2903206 ]\n",
      "[ 0.07592554 -0.48634264  1.2903206 ]\n",
      "[ 0.07592554 -0.48634264  1.2903206 ]\n",
      "\n",
      "[ 0.07592554 -0.48634264  1.2903206 ]\n",
      "[-0.7197971  1.5521808 -0.8557356]\n",
      "[ 1.5832745   0.06891013 -0.23817426]\n",
      "[ 0.16889559 -1.978758   -1.169645  ]\n",
      "[ 0.32114476  0.21683139 -0.9483882 ]\n",
      "[-0.69533247  0.40693274  0.3515198 ]\n",
      "[-1.0713949   0.20744139  1.0686921 ]\n",
      "[-1.490867    0.15006733 -0.24445076]\n",
      "[ 0.64465773 -1.0203602  -1.672845  ]\n",
      "[1.1716218  0.42618108 0.23057559]\n",
      "\n",
      "[ 0.07592554 -0.48634264  1.2903206 ]\n",
      "[ 0.60576403  0.7990441  -0.908927  ]\n",
      "[ 0.4323065  0.5872638 -1.1416743]\n",
      "[-0.2818947 -1.367489  -1.6350379]\n",
      "[0.6549178  0.17345214 1.6018405 ]\n",
      "[-0.2166012  -1.9878021  -0.61060226]\n",
      "[-0.25440374 -0.6385937  -0.68521845]\n",
      "[ 0.2886397  -0.00963292  0.15268941]\n",
      "[ 0.14384735 -0.15262456 -1.7989424 ]\n",
      "[-1.3462586   0.5520057  -0.75974613]\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.key(42)\n",
    "print(jax.random.normal(key, 3)) # generate 3 standard normal values\n",
    "\n",
    "print(jax.random.normal(key, 3)) # generate 3 standard normal values, because we are passing the same key, we get the same valeus\n",
    "\n",
    "key, _ = jax.random.split(key, 2) # split into two new \"child\" keys ... we just use the first one\n",
    "print(jax.random.normal(key, 3)) # generate 3 standard normal values, now the key is different so we get a diff value\n",
    "\n",
    "# this doesn't work, it will keep generating same value\n",
    "for i in range(10):\n",
    "    val = jax.random.normal(key, 3)\n",
    "    print(val)\n",
    "\n",
    "print()\n",
    "# a bit better but not ideal ...\n",
    "key = jax.random.key(42)\n",
    "for i in range(10):\n",
    "    key, _ = jax.random.split(key)\n",
    "    val = jax.random.normal(key, 3)\n",
    "    print(val)\n",
    "\n",
    "print()\n",
    "\n",
    "# good solution\n",
    "key = jax.random.key(42)\n",
    "for i in range(10):\n",
    "    loop_key = jax.random.fold_in(key, i)\n",
    "    val = jax.random.normal(loop_key, 3)\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db5d238",
   "metadata": {},
   "source": [
    "# LinearModel with AutoDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "76d6802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(Beta, X, y):\n",
    "    yhat = X @ Beta \n",
    "    mse = jnp.mean(jnp.square(yhat - y))\n",
    "    return mse \n",
    "\n",
    "class LinearModel():\n",
    "\n",
    "    def __init__(self, \n",
    "                 features: List[str]):\n",
    "        self._features = features \n",
    "    \n",
    "    def train(self, rng, df: pd.DataFrame, y: np.ndarray, steps: int = 100, eta: float = 0.01):\n",
    "\n",
    "        # prepare inputs and outputs\n",
    "        X = self._prepare_input_matrix(df[self._features])\n",
    "        \n",
    "        # randomly initialize solution \n",
    "        Beta = jax.random.normal(rng, X.shape[1]) # K\n",
    "\n",
    "        # iterate for steps\n",
    "        history = []\n",
    "        for i in range(steps):\n",
    "            mse = loss_fn(Beta, X, y)\n",
    "            history.append([Beta, mse])\n",
    "\n",
    "            # compute gradient\n",
    "            # this is very powerful ... JAX takes care of derivative computation\n",
    "            # so loss_fn could be as complex as you like\n",
    "            Beta_grad = jax.grad(loss_fn)(Beta, X, y)\n",
    "            \n",
    "            # update solution\n",
    "            Beta = Beta - eta * Beta_grad\n",
    "        \n",
    "        # save the parameters\n",
    "        self._params, _ = history[-1]\n",
    "\n",
    "        return history\n",
    "    \n",
    "    def _prepare_input_matrix(self, df: pd.DataFrame):\n",
    "\n",
    "        # we need to separate categorical from numeric features\n",
    "        # because they require separate processing\n",
    "        # let's get categorical columns\n",
    "        categorical_cols = df.select_dtypes(include='object').columns\n",
    "        \n",
    "        # let's get numeric\n",
    "        ordinal_cols = df.select_dtypes(include='number').columns\n",
    "\n",
    "        # construct input features\n",
    "        X = df[ordinal_cols].to_numpy()\n",
    "\n",
    "        # z-score (NxK' - 1xK') / 1xK' = NxK'\n",
    "        X = (X - np.mean(X, axis=0)[None, :]) / np.std(X, axis=0)[None, :]\n",
    "\n",
    "        # code categorical features\n",
    "        for feature in categorical_cols:\n",
    "            dummies = pd.get_dummies(df[feature]).to_numpy().astype(float)\n",
    "            X = np.hstack((X, dummies)) \n",
    "\n",
    "        # add a column of ones\n",
    "        ones_col = np.ones((X.shape[0], 1)) # Nx1\n",
    "        X = np.hstack((ones_col, X)) # K\n",
    "        \n",
    "        return jnp.array(X) \n",
    "    \n",
    "    def predict(self, df: pd.DataFrame):\n",
    "         \n",
    "        X = self._prepare_input_matrix(df[self._features])\n",
    "\n",
    "        # compute model predictions\n",
    "        yhat = X @ self._params # N\n",
    "\n",
    "        return yhat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2a8de1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  (406, 9)\n",
      "After:  (392, 9)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "df = pd.read_json('../data/cars.json')\n",
    "\n",
    "# Filter dataframe\n",
    "required_cols = ['Miles_per_Gallon', 'Cylinders', 'Displacement', 'Horsepower', 'Weight_in_lbs', 'Acceleration', 'Origin']\n",
    "\n",
    "# only include rows where ALL columns are not nan\n",
    "ix_included = np.sum(pd.isna(df[required_cols]), axis=1) == 0\n",
    "\n",
    "# exclude examples with no horsepower or mpg\n",
    "print(\"Before: \", df.shape)\n",
    "df = df[ix_included]\n",
    "print(\"After: \", df.shape)\n",
    "df = df[required_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca6547d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.227446\n",
      "25.227448\n",
      "[502.9719   474.6761   448.10617  423.15173  399.71002  377.68518\n",
      " 356.98804  337.5352   319.24896  302.05676  285.8908   270.68774\n",
      " 256.38837  242.93726  230.28256  218.3758   207.17148  196.6271\n",
      " 186.70279  177.36113  168.56714  160.28795  152.4927   145.15256\n",
      " 138.24034  131.73056  125.59935  119.82422  114.38408  109.25913\n",
      " 104.43077   99.88148   95.594826  91.55535   87.748566  84.16075\n",
      "  80.77911   77.59155   74.586716  71.7539    69.083084  66.56477\n",
      "  64.190094  61.950684  59.83864   57.846565  55.967457  54.19479\n",
      "  52.52233   50.94429   49.455196  48.049873  46.72346   45.471416\n",
      "  44.289402  43.173397  42.119564  41.124313  40.18426   39.296223\n",
      "  38.457195  37.664352  36.915043  36.20674   35.53711   34.90389\n",
      "  34.30502   33.738506  33.202503  32.695244  32.215088  31.760492\n",
      "  31.32998   30.922186  30.535795  30.169598  29.822428  29.493214\n",
      "  29.180918  28.884594  28.603315  28.336246  28.082563  27.841513\n",
      "  27.61239   27.394506  27.187233  26.989962  26.802143  26.623243\n",
      "  26.45276   26.290209  26.135164  25.987194  25.845911  25.71094\n",
      "  25.58193   25.458548  25.340487  25.227446]\n"
     ]
    }
   ],
   "source": [
    "# Let's test the new class \n",
    "model = LinearModel(['Displacement', 'Origin', 'Cylinders', 'Horsepower'])\n",
    "rng = jax.random.key(52345)\n",
    "\n",
    "history = model.train(rng, df, df[ 'Miles_per_Gallon'].to_numpy())\n",
    "print(history[-1][1])\n",
    "yhat = model.predict(df)\n",
    "mse = np.mean(np.square(yhat - df.Miles_per_Gallon))\n",
    "print(mse)\n",
    "print(jnp.array([h[1] for h in history]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c8439e",
   "metadata": {},
   "source": [
    "## Non-Linear Model\n",
    "\n",
    "Let's make a slight change to the model, so that it is non-linear:\n",
    "\n",
    "$f(\\mathbf{x}_{i})=\\text{exp}(b_{0}+b_{1}x_{i1}+...)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e128ef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(Beta, X, y):\n",
    "    yhat = X @ Beta \n",
    "    f = jnp.exp(yhat) # <-- that's the change\n",
    "    mse = jnp.mean(jnp.square(f - y))\n",
    "    return mse \n",
    "\n",
    "class NonLinearModel():\n",
    "\n",
    "    def __init__(self, \n",
    "                 features: List[str]):\n",
    "        self._features = features \n",
    "    \n",
    "    def train(self, rng, df: pd.DataFrame, y: np.ndarray, steps: int = 100, eta: float = 0.01):\n",
    "\n",
    "        # prepare inputs and outputs\n",
    "        X = self._prepare_input_matrix(df[self._features])\n",
    "        \n",
    "        # randomly initialize solution \n",
    "        Beta = jax.random.normal(rng, X.shape[1]) # K\n",
    "\n",
    "        # iterate for steps\n",
    "        history = []\n",
    "        for i in range(steps):\n",
    "            mse = loss_fn(Beta, X, y)\n",
    "            history.append([Beta, mse])\n",
    "\n",
    "            # compute gradient\n",
    "            # this is very powerful ... JAX takes care of derivative computation\n",
    "            # so loss_fn could be as complex as you like\n",
    "            Beta_grad = jax.grad(loss_fn)(Beta, X, y)\n",
    "            \n",
    "            # update solution\n",
    "            Beta = Beta - eta * Beta_grad\n",
    "        \n",
    "        # save the parameters\n",
    "        self._params, _ = history[-1]\n",
    "\n",
    "        return history\n",
    "    \n",
    "    def _prepare_input_matrix(self, df: pd.DataFrame):\n",
    "\n",
    "        # we need to separate categorical from numeric features\n",
    "        # because they require separate processing\n",
    "        # let's get categorical columns\n",
    "        categorical_cols = df.select_dtypes(include='object').columns\n",
    "        \n",
    "        # let's get numeric\n",
    "        ordinal_cols = df.select_dtypes(include='number').columns\n",
    "\n",
    "        # construct input features\n",
    "        X = df[ordinal_cols].to_numpy()\n",
    "\n",
    "        # z-score (NxK' - 1xK') / 1xK' = NxK'\n",
    "        X = (X - np.mean(X, axis=0)[None, :]) / (10 * np.std(X, axis=0)[None, :])\n",
    "\n",
    "        # code categorical features\n",
    "        for feature in categorical_cols:\n",
    "            dummies = pd.get_dummies(df[feature]).to_numpy().astype(float)\n",
    "            X = np.hstack((X, dummies)) \n",
    "\n",
    "        # add a column of ones\n",
    "        ones_col = np.ones((X.shape[0], 1)) # Nx1\n",
    "        X = np.hstack((ones_col, X)) # K\n",
    "        \n",
    "        return jnp.array(X) \n",
    "    \n",
    "    def predict(self, df: pd.DataFrame):\n",
    "         \n",
    "        X = self._prepare_input_matrix(df[self._features])\n",
    "\n",
    "        # compute model predictions\n",
    "        yhat = X @ self._params # N\n",
    "        f = jnp.exp(yhat)\n",
    "        \n",
    "        return f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1d904ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.398119\n",
      "18.398119\n",
      "[258.71008  169.6359   109.90535   87.903564  71.76885   60.84645\n",
      "  52.93802   46.50946   41.01135   36.421986  32.795033  30.108097\n",
      "  28.232958  26.981155  26.159897  25.611547  25.225342  24.931568\n",
      "  24.689592  24.47699   24.281885  24.098013  23.92216   23.752663\n",
      "  23.588598  23.429468  23.274944  23.124804  22.978876  22.837032\n",
      "  22.699104  22.565008  22.434599  22.307781  22.184439  22.06447\n",
      "  21.947779  21.83427   21.723827  21.61639   21.511852  21.410141\n",
      "  21.311144  21.214815  21.121061  21.029789  20.940962  20.854477\n",
      "  20.77029   20.688316  20.608498  20.530773  20.455074  20.38135\n",
      "  20.309538  20.239588  20.171442  20.105053  20.04037   19.977337\n",
      "  19.915909  19.856047  19.797697  19.740828  19.685379  19.631336\n",
      "  19.578636  19.52725   19.477135  19.428267  19.380606  19.334114\n",
      "  19.288769  19.244518  19.20135   19.15923   19.118124  19.07801\n",
      "  19.038853  19.000639  18.963333  18.92691   18.891354  18.856627\n",
      "  18.822714  18.789597  18.757248  18.72565   18.694782  18.664623\n",
      "  18.63515   18.606352  18.578215  18.55071   18.523827  18.497538\n",
      "  18.471842  18.446718  18.422148  18.398119]\n"
     ]
    }
   ],
   "source": [
    "# Let's test the new class \n",
    "model = NonLinearModel(['Displacement', 'Origin', 'Cylinders', 'Horsepower'])\n",
    "\n",
    "rng = jax.random.key(52345)\n",
    "\n",
    "history = model.train(rng, df, df[ 'Miles_per_Gallon'].to_numpy(), eta=0.001)\n",
    "print(history[-1][1])\n",
    "yhat = model.predict(df)\n",
    "mse = np.mean(np.square(yhat - df.Miles_per_Gallon))\n",
    "print(mse)\n",
    "print(jnp.array([h[1] for h in history]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4d662e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-ml-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
